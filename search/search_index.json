{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Karpenter IBM Cloud Provider","text":"Automatic Node Provisioning for Kubernetes on IBM Cloud VPC"},{"location":"#overview","title":"Overview","text":"<p>The Karpenter IBM Cloud Provider enables automatic node provisioning and scaling for Kubernetes clusters running on IBM Cloud VPC infrastructure. It implements the Karpenter provisioning APIs to provide just-in-time compute resources based on application demands.</p> <ul> <li> <p> Getting Started   Get up and running with Karpenter on IBM Cloud in minutes</p> </li> <li> <p> VPC Integration   Deploy on self-managed Kubernetes clusters with full control</p> </li> <li> <p> IKS Integration   Seamless integration with IBM Kubernetes Service</p> </li> <li> <p> Configuration   Learn about bootstrap methods and configuration options</p> </li> <li> <p> Nightly Builds   Access bleeding-edge features and development versions</p> </li> <li> <p> Presentations   Conference talks and demos about the project</p> </li> <li> <p> FAQ   Frequently asked questions about Karpenter IBM Cloud Provider</p> </li> </ul>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#automatic-node-provisioning","title":"Automatic Node Provisioning","text":"<ul> <li>Just-in-Time Compute: Provisions nodes automatically when pods are unschedulable</li> <li>Intelligent Instance Selection: Chooses optimal instance types based on workload requirements</li> <li>Multi-Zone Support: Distributes nodes across availability zones for high availability</li> </ul>"},{"location":"#zero-configuration-bootstrap","title":"Zero-Configuration Bootstrap","text":"<ul> <li>Automatic Cluster Discovery: Detects cluster configuration without manual setup</li> <li>Dynamic Bootstrap Scripts: Generates appropriate initialization scripts</li> <li>Multiple Bootstrap Modes: Supports VPC, IKS, and automatic mode selection</li> </ul>"},{"location":"#cost-optimization","title":"Cost Optimization","text":"<ul> <li>Node Consolidation: Automatically removes underutilized nodes</li> <li>Right-Sizing: Selects appropriate instance types to minimize costs</li> </ul>"},{"location":"#architecture","title":"Architecture","text":"<p>The Karpenter IBM Cloud Provider consists of several key components:</p> <pre><code>graph TB\n    subgraph \"Kubernetes Cluster\"\n        K[Karpenter Controller]\n        NP[NodePool]\n        NC[NodeClaim]\n        INC[IBMNodeClass]\n        P[Pending Pods]\n    end\n\n    subgraph \"IBM Cloud\"\n        VPC[VPC Infrastructure]\n        IKS[IKS Service]\n        VSI[Virtual Server Instances]\n    end\n\n    P --&gt;|Triggers| K\n    K --&gt;|Creates| NC\n    K --&gt;|Reads| NP\n    K --&gt;|Reads| INC\n    NC --&gt;|Provisions| VSI\n    VSI --&gt;|Joins| K\n    INC -.-&gt;|Config| VPC\n    INC -.-&gt;|Config| IKS</code></pre>"},{"location":"#deployment-options","title":"Deployment Options","text":""},{"location":"#vpc-self-managed-clusters","title":"VPC Self-Managed Clusters","text":"<ul> <li>Full control over Kubernetes configuration</li> <li>Dynamic instance type selection</li> <li>Custom bootstrap configurations</li> <li>Ideal for specialized workloads</li> </ul>"},{"location":"#ibm-kubernetes-service-iks","title":"IBM Kubernetes Service (IKS)","text":"<ul> <li>Seamless integration with managed service</li> <li>Worker pool expansion</li> </ul>"},{"location":"#community","title":"Community","text":"<ul> <li>GitHub: Report issues and contribute</li> <li>Slack: Join #karpenter-ibm on CNCF Slack</li> </ul>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the Apache License 2.0. See the LICENSE file for details.</p>"},{"location":"README_DOCS/","title":"Documentation","text":"<p>This directory contains the documentation for the Karpenter IBM Cloud Provider, built with MkDocs and the Material theme.</p>"},{"location":"README_DOCS/#local-development","title":"Local Development","text":""},{"location":"README_DOCS/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8+</li> <li>pip</li> </ul>"},{"location":"README_DOCS/#setup","title":"Setup","text":"<ol> <li> <p>Install dependencies: <pre><code>pip install -r docs/requirements.txt\n</code></pre></p> </li> <li> <p>Serve documentation locally: <pre><code>mkdocs serve\n</code></pre></p> </li> <li> <p>View at http://localhost:8000</p> </li> </ol>"},{"location":"README_DOCS/#building","title":"Building","text":"<p>Build static site: <pre><code>mkdocs build\n</code></pre></p> <p>Output will be in <code>site/</code> directory.</p>"},{"location":"README_DOCS/#deployment","title":"Deployment","text":"<p>Documentation is automatically deployed to GitHub Pages when changes are pushed to the <code>main</code> branch. The deployment workflow:</p> <ol> <li>Builds documentation with MkDocs</li> <li>Deploys to GitHub Pages alongside Helm charts</li> <li>Available at https://pfeifferj.github.io/karpenter-provider-ibm-cloud/</li> </ol>"},{"location":"README_DOCS/#writing-documentation","title":"Writing Documentation","text":""},{"location":"README_DOCS/#style-guide","title":"Style Guide","text":"<ul> <li>Use clear, concise language</li> <li>Include code examples for all configurations</li> <li>Use admonitions for important notes:</li> <li><code>!!! note</code> for general information</li> <li><code>!!! warning</code> for important warnings</li> <li><code>!!! danger</code> for critical warnings</li> <li><code>!!! success</code> for success messages</li> <li><code>!!! info</code> for informational content</li> </ul>"},{"location":"README_DOCS/#adding-pages","title":"Adding Pages","text":"<ol> <li>Create new <code>.md</code> file in <code>docs/</code></li> <li>Add to navigation in <code>mkdocs.yml</code></li> <li>Follow existing page structure</li> </ol>"},{"location":"README_DOCS/#code-examples","title":"Code Examples","text":"<p>Use fenced code blocks with language hints:</p> <pre><code>```yaml\napiVersion: karpenter-ibm.sh/v1alpha1\nkind: IBMNodeClass\n...\n```\n</code></pre>"},{"location":"README_DOCS/#diagrams","title":"Diagrams","text":"<p>Use Mermaid for diagrams:</p> <pre><code>```mermaid\ngraph LR\n    A[Start] --&gt; B[Process]\n    B --&gt; C[End]\n```\n</code></pre>"},{"location":"README_DOCS/#contributing","title":"Contributing","text":"<ol> <li>Fork the repository</li> <li>Create a feature branch</li> <li>Make documentation changes</li> <li>Test locally with <code>mkdocs serve</code></li> <li>Submit pull request</li> </ol>"},{"location":"README_DOCS/#resources","title":"Resources","text":"<ul> <li>MkDocs Documentation</li> <li>Material for MkDocs</li> <li>Markdown Guide</li> </ul>"},{"location":"block-device-mapping/","title":"Block Device Mapping","text":"<p>Block Device Mapping allows you to customize the storage configuration for instances provisioned by Karpenter. This feature provides fine-grained control over boot volumes and additional data volumes, similar to AWS Karpenter's block device mapping functionality.</p>"},{"location":"block-device-mapping/#overview","title":"Overview","text":"<p>By default, Karpenter provisions instances with a 100GB general-purpose boot volume. With Block Device Mapping, you can:</p> <ul> <li>Customize boot volume size and performance (capacity, IOPS, bandwidth)</li> <li>Add additional data volumes with specific configurations</li> <li>Configure encryption using IBM Key Protect or Hyper Protect Crypto Services</li> <li>Control volume lifecycle (delete on termination or persist)</li> <li>Apply custom tags for organization and billing</li> </ul>"},{"location":"block-device-mapping/#configuration","title":"Configuration","text":"<p>Block device mappings are configured in the <code>IBMNodeClass</code> specification using the <code>blockDeviceMappings</code> field:</p> <pre><code>apiVersion: karpenter-ibm.sh/v1alpha1\nkind: IBMNodeClass\nmetadata:\n  name: custom-storage\nspec:\n  # ... other IBMNodeClass fields ...\n  blockDeviceMappings:\n    - rootVolume: true\n      volumeSpec:\n        capacity: 200\n        profile: \"10iops-tier\"\n    - deviceName: \"data-storage\"\n      volumeSpec:\n        capacity: 500\n        profile: \"5iops-tier\"\n</code></pre>"},{"location":"block-device-mapping/#field-reference","title":"Field Reference","text":""},{"location":"block-device-mapping/#blockdevicemapping","title":"<code>BlockDeviceMapping</code>","text":"Field Type Required Description <code>deviceName</code> <code>string</code> No Custom name for the volume attachment. Auto-generated if not specified. <code>volumeSpec</code> <code>VolumeSpec</code> No Volume configuration. Uses defaults for root volumes if not specified. <code>rootVolume</code> <code>boolean</code> No Indicates if this is the boot/root volume. Only one mapping can have this set to <code>true</code>."},{"location":"block-device-mapping/#volumespec","title":"<code>VolumeSpec</code>","text":"Field Type Required Description <code>capacity</code> <code>int64</code> No Volume size in GB. Range: 1-16000 GB. Boot volumes max: 250 GB. <code>profile</code> <code>string</code> No Storage profile. Options: <code>general-purpose</code>, <code>5iops-tier</code>, <code>10iops-tier</code>, <code>custom</code>. <code>iops</code> <code>int64</code> No I/O operations per second. Only valid for <code>custom</code> profile. Range: 100-64000. <code>bandwidth</code> <code>int64</code> No Maximum bandwidth in Mbps. Range: 1-16000. Auto-calculated if not specified. <code>encryptionKeyID</code> <code>string</code> No CRN of customer root key for encryption. Uses provider-managed encryption if not specified. <code>deleteOnTermination</code> <code>boolean</code> No Whether to delete volume when instance terminates. Default: <code>true</code>. <code>tags</code> <code>[]string</code> No User tags to apply to the volume. Maximum: 10 tags."},{"location":"block-device-mapping/#ibm-cloud-storage-profiles","title":"IBM Cloud Storage Profiles","text":"<p>IBM Cloud VPC offers different storage profiles optimized for various workloads:</p>"},{"location":"block-device-mapping/#standard-profiles","title":"Standard Profiles","text":"Profile Description IOPS/GB Max IOPS Use Case <code>general-purpose</code> Balanced performance and cost 3 16,000 General workloads <code>5iops-tier</code> Higher performance 5 48,000 I/O intensive applications <code>10iops-tier</code> High performance 10 48,000 Database workloads"},{"location":"block-device-mapping/#custom-profile","title":"Custom Profile","text":"Profile Description IOPS Range Bandwidth Range <code>custom</code> User-defined performance 100-64,000 Varies by capacity and IOPS"},{"location":"block-device-mapping/#examples","title":"Examples","text":""},{"location":"block-device-mapping/#basic-custom-boot-volume","title":"Basic Custom Boot Volume","text":"<pre><code>apiVersion: karpenter-ibm.sh/v1alpha1\nkind: IBMNodeClass\nmetadata:\n  name: large-boot-volume\nspec:\n  region: us-south\n  instanceProfile: bx2-4x16\n  image: ubuntu-24-04-amd64\n  vpc: r010-12345678-1234-5678-9abc-def012345678\n  subnet: 0717-197e06f4-b500-426c-bc0f-900b215f996c\n  securityGroups:\n    - r010-87654321-4321-8765-cba9-876543210fed\n\n  blockDeviceMappings:\n    - rootVolume: true\n      volumeSpec:\n        capacity: 200                    # 200GB boot volume\n        profile: \"10iops-tier\"          # High performance\n        deleteOnTermination: true       # Clean up on termination\n</code></pre>"},{"location":"block-device-mapping/#multiple-data-volumes","title":"Multiple Data Volumes","text":"<pre><code>blockDeviceMappings:\n  # Custom boot volume\n  - rootVolume: true\n    volumeSpec:\n      capacity: 100\n      profile: \"general-purpose\"\n\n  # Application storage\n  - deviceName: \"app-data\"\n    volumeSpec:\n      capacity: 500\n      profile: \"5iops-tier\"\n      deleteOnTermination: false      # Persist data\n      tags:\n        - \"application-storage\"\n\n  # Database storage with custom IOPS\n  - deviceName: \"database\"\n    volumeSpec:\n      capacity: 1000\n      profile: \"custom\"\n      iops: 8000                      # Custom IOPS\n      bandwidth: 400                  # Custom bandwidth\n      deleteOnTermination: false\n      encryptionKeyID: \"crn:v1:bluemix:public:kms:...\"\n      tags:\n        - \"database\"\n        - \"high-performance\"\n</code></pre>"},{"location":"block-device-mapping/#default-behavior","title":"Default Behavior","text":"<p>When <code>blockDeviceMappings</code> is not specified or empty:</p> <ul> <li>Boot Volume: 100GB general-purpose storage</li> <li>Delete on Termination: <code>true</code></li> <li>Profile: <code>general-purpose</code></li> <li>Encryption: Provider-managed</li> <li>Data Volumes: None</li> </ul>"},{"location":"bootstrap-methods/","title":"Bootstrap Methods","text":"<p>The Karpenter IBM Cloud Provider provides automatic node bootstrap capabilities to seamlessly join IBM Cloud VPC instances to your Kubernetes cluster. This document explains the available bootstrap methods and their configurations.</p>"},{"location":"bootstrap-methods/#overview","title":"Overview","text":"<p>The provider supports three bootstrap approaches:</p> <ol> <li>Auto Bootstrap (Default) - Intelligent automatic method selection (Experimental)</li> <li>VPC Bootstrap - Direct cloud-init integration for self-managed clusters</li> <li>IKS Bootstrap - Native IBM Kubernetes Service integration (Experimental)</li> </ol> <p>The provider aims to automatically detect your cluster configuration and generates appropriate bootstrap scripts with no manual userData needed.</p>"},{"location":"bootstrap-methods/#auto-bootstrap-experimental","title":"Auto Bootstrap (Experimental)","text":""},{"location":"bootstrap-methods/#when-to-use","title":"When to Use","text":"<ul> <li>Simplified configuration without manual bootstrap decisions</li> </ul>"},{"location":"bootstrap-methods/#how-it-works","title":"How It Works","text":"<p>Auto Bootstrap should select the right bootstrap method based on your configuration:</p> <ol> <li>IKS Detection: If <code>iksClusterID</code> is provided and accessible \u2192 Uses IKS Bootstrap</li> <li>VPC Fallback: Otherwise \u2192 Uses VPC Bootstrap with automatic cluster discovery</li> </ol>"},{"location":"bootstrap-methods/#configuration","title":"Configuration","text":"<pre><code>apiVersion: karpenter-ibm.sh/v1alpha1\nkind: IBMNodeClass\nmetadata:\n  name: auto-bootstrap-nodeclass\nspec:\n  region: us-south\n  zone: us-south-1\n  vpc: vpc-12345678\n  image: r006-ubuntu-20-04\n\n  # Auto bootstrap (default - no bootstrapMode needed)\n  # Optionally provide IKS cluster ID for IKS preference\n  iksClusterID: \"cluster-12345678\"  # Optional\n\n  # No userData required - fully automatic!\n</code></pre>"},{"location":"bootstrap-methods/#automatic-features","title":"Automatic Features","text":"<ul> <li>Cluster Discovery: Automatically detects cluster API endpoint and CA certificate</li> <li>Token Management: Generates and refreshes bootstrap tokens automatically with generic RBAC</li> <li>Network Detection: Discovers cluster CIDR and DNS configuration</li> <li>System Configuration: Enables IP forwarding, disables swap, configures hostname</li> <li>Runtime Selection: Auto-detects and configures container runtime (containerd/crio)</li> </ul>"},{"location":"bootstrap-methods/#bootstrap-token-rbac-design","title":"Bootstrap Token RBAC Design","text":"<p>The provider uses a generic RBAC approach for bootstrap tokens:</p>"},{"location":"bootstrap-methods/#single-role-for-all-nodepools","title":"Single Role for All NodePools","text":"<pre><code># All bootstrap tokens use the same generic group\ngroup: \"system:bootstrappers:karpenter:ibm-cloud\"\n\n# Single ClusterRoleBinding covers all NodePools\nsubjects:\n- kind: Group\n  name: system:bootstrappers:karpenter:ibm-cloud\n  apiGroup: rbac.authorization.k8s.io\n</code></pre>"},{"location":"bootstrap-methods/#vpc-bootstrap-cloud-init","title":"VPC Bootstrap (Cloud-Init)","text":""},{"location":"bootstrap-methods/#when-to-use_1","title":"When to Use","text":"<ul> <li>Self-managed Kubernetes clusters running on IBM Cloud VPC</li> <li>Custom cluster configurations requiring specific setup</li> </ul>"},{"location":"bootstrap-methods/#configuration_1","title":"Configuration","text":"<pre><code>apiVersion: karpenter-ibm.sh/v1alpha1\nkind: IBMNodeClass\nmetadata:\n  name: vpc-bootstrap-nodeclass\nspec:\n  region: us-south\n  zone: us-south-1\n  vpc: vpc-12345678\n  image: r006-ubuntu-20-04\n\n  # Explicit VPC bootstrap mode (optional - auto-detected)\n  bootstrapMode: vpc\n\n  # Optional custom pre-bootstrap setup\n  userData: |\n    #!/bin/bash\n    echo \"Custom pre-bootstrap configuration\"\n    # Bootstrap script automatically appended\n</code></pre>"},{"location":"bootstrap-methods/#automatic-features_1","title":"Automatic Features","text":""},{"location":"bootstrap-methods/#intelligent-cluster-discovery","title":"Intelligent Cluster Discovery","text":"<ul> <li>API Endpoint Detection: Automatically finds internal cluster API server endpoint</li> <li>Certificate Authority: Extracts cluster CA certificate from existing nodes</li> <li>DNS Configuration: Discovers cluster DNS service IP and domain</li> <li>Network Setup: Detects cluster pod and service CIDR ranges</li> </ul>"},{"location":"bootstrap-methods/#container-runtime-management","title":"Container Runtime Management","text":"<ul> <li>Containerd (Default): Installs and configures containerd runtime</li> <li>CRI-O Support: Alternative container runtime option</li> <li>Auto-Detection: Analyzes existing cluster nodes to match runtime</li> </ul>"},{"location":"bootstrap-methods/#cni-plugin-integration","title":"CNI Plugin Integration","text":"<ul> <li>Calico: Full support with automatic configuration</li> <li>Cilium: Advanced networking with eBPF support</li> <li>Flannel: Lightweight overlay networking</li> <li>Auto-Detection: Matches CNI plugin used by existing cluster nodes</li> </ul>"},{"location":"bootstrap-methods/#complete-kubernetes-setup","title":"Complete Kubernetes Setup","text":"<ul> <li>System Preparation: Configures system requirements (swap, IP forwarding, hostname)</li> <li>Package Installation: Installs kubelet, kubeadm, kubectl with correct versions</li> <li>Service Configuration: Sets up systemd services and startup scripts</li> <li>Node Labeling: Applies proper Karpenter and workload labels</li> <li>Bootstrap Process: Executes kubeadm join with proper configuration</li> </ul>"},{"location":"bootstrap-methods/#customization-options","title":"Customization Options","text":""},{"location":"bootstrap-methods/#custom-user-data","title":"Custom User Data","text":"<pre><code>spec:\n  userData: |\n    #!/bin/bash\n    # Your custom pre-bootstrap configuration\n    echo \"Installing custom packages...\"\n    apt-get update &amp;&amp; apt-get install -y htop vim\n\n    # Custom environment variables\n    echo \"CUSTOM_VAR=value\" &gt;&gt; /etc/environment\n\n    # Custom service configuration\n    systemctl enable my-custom-service\n</code></pre>"},{"location":"bootstrap-methods/#environment-variables","title":"Environment Variables","text":"<pre><code># Override container runtime\nexport CONTAINER_RUNTIME=crio\n\n# Custom CNI configuration\nexport CNI_PLUGIN=cilium\n\n# Debug mode\nexport DEBUG=true\n</code></pre>"},{"location":"bootstrap-methods/#iks-bootstrap-experimental","title":"IKS Bootstrap (Experimental)","text":""},{"location":"bootstrap-methods/#when-to-use_2","title":"When to Use","text":"<ul> <li>IBM Kubernetes Service (IKS) clusters with existing worker pools</li> <li>Consistent worker pool management across teams</li> </ul>"},{"location":"bootstrap-methods/#configuration_2","title":"Configuration","text":"<pre><code>apiVersion: karpenter-ibm.sh/v1alpha1\nkind: IBMNodeClass\nmetadata:\n  name: iks-bootstrap-nodeclass\nspec:\n  region: us-south\n  zone: us-south-1\n  vpc: vpc-iks-12345\n  image: r006-ubuntu-20-04\n\n  # IKS-specific configuration\n  iksClusterID: \"cluster-12345678\"        # Required: Your IKS cluster ID\n  iksWorkerPoolID: \"pool-default\"         # Optional: specific worker pool\n\n  # Optional: Custom post-registration setup\n  userData: |\n    #!/bin/bash\n    echo \"Post-IKS registration customization\"\n    # Custom configuration after node joins IKS cluster\n</code></pre>"},{"location":"bootstrap-methods/#features","title":"Features","text":""},{"location":"bootstrap-methods/#native-iks-integration","title":"Native IKS Integration","text":"<ul> <li>Worker Pool API: Uses IBM Kubernetes Service worker pool resize APIs</li> <li>Automatic Registration: Nodes automatically join IKS cluster through worker pools</li> </ul>"},{"location":"bootstrap-methods/#important-constraints","title":"Important Constraints","text":""},{"location":"bootstrap-methods/#instance-type-limitations","title":"Instance Type Limitations","text":"<ul> <li>Constraint: Cannot dynamically select instance types in IKS mode</li> <li>Reason: IKS Worker Pool API uses pre-configured instance types</li> <li>Impact: <code>instanceProfile</code> and <code>instanceRequirements</code> are ignored</li> <li>Solution: Pre-create worker pools for different instance types</li> </ul> <pre><code># Example: Multiple NodeClasses for different instance types\n---\napiVersion: karpenter-ibm.sh/v1alpha1\nkind: IBMNodeClass\nmetadata:\n  name: iks-small-instances\nspec:\n  iksClusterID: \"cluster-12345678\"\n  iksWorkerPoolID: \"pool-small\"     # Pre-configured with bx2-2x8\n---\napiVersion: karpenter-ibm.sh/v1alpha1\nkind: IBMNodeClass\nmetadata:\n  name: iks-large-instances\nspec:\n  iksClusterID: \"cluster-12345678\"\n  iksWorkerPoolID: \"pool-large\"     # Pre-configured with bx2-8x32\n</code></pre>"},{"location":"bootstrap-methods/#requirements","title":"Requirements","text":""},{"location":"bootstrap-methods/#iks-cluster-access","title":"IKS Cluster Access","text":"<ul> <li>Valid IKS cluster ID in same region as nodes</li> <li>API key with IKS cluster access permissions</li> <li>Worker pools pre-configured with desired instance types</li> </ul>"},{"location":"bootstrap-methods/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"bootstrap-methods/#environment-variables_1","title":"Environment Variables","text":"<p>All bootstrap modes support environment variable customization:</p> <pre><code># Bootstrap behavior\nexport LOG_LEVEL=debug                   # Enhanced logging\nexport BOOTSTRAP_TIMEOUT=600             # Bootstrap timeout in seconds\n\n# Container runtime preferences\nexport CONTAINER_RUNTIME=containerd      # or crio\nexport CONTAINERD_VERSION=1.7.22         # Specific version\n\n# CNI plugin preferences\nexport CNI_PLUGIN=calico                 # or cilium, flannel\nexport CNI_VERSION=v3.29.0               # Specific CNI version\n\n# System configuration\nexport ENABLE_IP_FORWARDING=true         # Enable IP forwarding\nexport DISABLE_SWAP=true                 # Disable swap\nexport HOSTNAME_STRATEGY=ibm-cloud       # Hostname configuration strategy\n</code></pre>"},{"location":"bootstrap-methods/#troubleshooting-bootstrap-issues","title":"Troubleshooting Bootstrap Issues","text":""},{"location":"bootstrap-methods/#common-problems-and-solutions","title":"Common Problems and Solutions","text":""},{"location":"bootstrap-methods/#bootstrap-script-debugging","title":"Bootstrap Script Debugging","text":"<pre><code># Check cloud-init logs on the instance\nssh ubuntu@&lt;instance-ip&gt; \"sudo journalctl -u cloud-final\"\nssh ubuntu@&lt;instance-ip&gt; \"sudo tail -f /var/log/cloud-init-output.log\"\n\n# View the generated bootstrap script\nssh ubuntu@&lt;instance-ip&gt; \"sudo cat /var/lib/cloud/instance/scripts/*\"\n\n# Check bootstrap script execution status\nssh ubuntu@&lt;instance-ip&gt; \"sudo systemctl status cloud-final\"\n</code></pre>"},{"location":"bootstrap-methods/#cluster-join-failures","title":"Cluster Join Failures","text":"<p>VPC Clusters - Wrong API Endpoint (Most Common): <pre><code># Symptoms: Timeout errors, nodes never register\n# Check if using correct INTERNAL endpoint, not external\n\n# 1. Find correct internal API endpoint\nkubectl get endpointslice -n default -l kubernetes.io/service-name=kubernetes\n\n# 2. Update NodeClass with internal endpoint\nkubectl patch ibmnodeclass your-nodeclass --type='merge' \\\n  -p='{\"spec\":{\"apiServerEndpoint\":\"https://&lt;INTERNAL-IP&gt;:6443\"}}'\n\n# 3. Verify connectivity from worker instance\nssh ubuntu@&lt;instance-ip&gt; \"telnet &lt;INTERNAL-IP&gt; 6443\"\n</code></pre></p> <p>Bootstrap Token Issues: <pre><code># Check if bootstrap tokens are being created\nkubectl get secrets -n kube-system | grep bootstrap-token\n\n# Verify RBAC permissions exist\nkubectl get clusterrolebindings | grep karpenter-ibm-bootstrap-nodes\n\n# Check token authentication on instance\nssh ubuntu@&lt;instance-ip&gt; \"sudo cat /var/lib/kubelet/bootstrap-kubeconfig\"\n</code></pre></p> <p>General Debugging: <pre><code># Check kubelet status and logs\nssh ubuntu@&lt;instance-ip&gt; \"sudo systemctl status kubelet\"\nssh ubuntu@&lt;instance-ip&gt; \"sudo journalctl -u kubelet --no-pager -n 50\"\n\n# Verify cluster connectivity (use INTERNAL endpoint)\nssh ubuntu@&lt;instance-ip&gt; \"curl -k https://&lt;INTERNAL-IP&gt;:6443/healthz\"\n\n# For direct kubelet bootstrap (not kubeadm)\nssh ubuntu@&lt;instance-ip&gt; \"sudo journalctl -u kubelet | grep -E '(bootstrap|token|certificate)'\"\n</code></pre></p>"},{"location":"bootstrap-methods/#network-connectivity-issues","title":"Network Connectivity Issues","text":"<pre><code># Test DNS resolution\nssh ubuntu@&lt;instance-ip&gt; \"nslookup kubernetes.default.svc.cluster.local\"\n\n# Check if required ports are accessible\nssh ubuntu@&lt;instance-ip&gt; \"nc -zv CLUSTER_ENDPOINT 6443\"\n\n# Verify security group rules allow cluster communication\nibmcloud is security-group &lt;security-group-id&gt; --output json\n</code></pre>"},{"location":"circuit-breaker/","title":"Circuit Breaker Protection","text":""},{"location":"circuit-breaker/#overview","title":"Overview","text":"<p>The Karpenter IBM Cloud Provider includes a circuit breaker implementation to prevent scale-up storms and protect against cascading failures during node provisioning. The circuit breaker is configurable through Helm values, environment variables, and ConfigMaps.</p>"},{"location":"circuit-breaker/#problem-statement","title":"Problem Statement","text":"<p>In production environments, bootstrap failures or API issues can cause Karpenter to continuously create new instances that fail to join the cluster.</p>"},{"location":"circuit-breaker/#circuit-breaker-implementation","title":"Circuit Breaker Implementation","text":""},{"location":"circuit-breaker/#core-components","title":"Core Components","text":"<p>The circuit breaker is implemented in <code>/pkg/cloudprovider/circuitbreaker.go</code> and provides:</p> <ol> <li>State Management: CLOSED \u2192 OPEN \u2192 HALF_OPEN transitions</li> <li>Rate Limiting: Maximum instances per minute</li> <li>Concurrency Control: Maximum concurrent provisioning operations</li> <li>Failure Detection: Configurable failure thresholds</li> <li>Automatic Recovery: Time-based recovery with testing</li> </ol>"},{"location":"circuit-breaker/#configuration","title":"Configuration","text":"<pre><code>type CircuitBreakerConfig struct {\n    FailureThreshold       int           // 3 consecutive failures\n    FailureWindow          time.Duration // Within 5 minutes\n    RecoveryTimeout        time.Duration // Wait 15 minutes before retry\n    HalfOpenMaxRequests    int           // Allow 2 test requests\n    RateLimitPerMinute     int           // Max 2 instances/minute\n    MaxConcurrentInstances int           // Max 5 concurrent provisions\n}\n</code></pre>"},{"location":"circuit-breaker/#default-configuration","title":"Default Configuration","text":"<pre><code>func DefaultCircuitBreakerConfig() *CircuitBreakerConfig {\n    return &amp;CircuitBreakerConfig{\n        FailureThreshold:       3,                // 3 consecutive failures\n        FailureWindow:          5 * time.Minute,  // Within 5 minutes\n        RecoveryTimeout:        15 * time.Minute, // Wait 15 minutes before retry\n        HalfOpenMaxRequests:    2,                // Allow 2 test requests\n        RateLimitPerMinute:     2,                // Max 2 instances/minute\n        MaxConcurrentInstances: 5,                // Max 5 concurrent provisions\n    }\n}\n</code></pre>"},{"location":"circuit-breaker/#configuration-options","title":"Configuration Options","text":""},{"location":"circuit-breaker/#helm-values","title":"Helm Values","text":"<p>Configure the circuit breaker in your <code>values.yaml</code>:</p> <pre><code>circuitBreaker:\n  enabled: true\n\n  # Use a preset configuration\n  preset: \"balanced\"  # Options: conservative, balanced, aggressive, demo, custom\n\n  # Custom configuration (when preset: \"custom\")\n  config:\n    failureThreshold: 3\n    failureWindow: \"5m\"\n    recoveryTimeout: \"15m\"\n    halfOpenMaxRequests: 2\n    rateLimitPerMinute: 10\n    maxConcurrentInstances: 5\n</code></pre>"},{"location":"circuit-breaker/#environment-variables","title":"Environment Variables","text":"<p>Configure using environment variables:</p> <pre><code>export CIRCUIT_BREAKER_ENABLED=true\nexport CIRCUIT_BREAKER_FAILURE_THRESHOLD=3\nexport CIRCUIT_BREAKER_FAILURE_WINDOW=5m\nexport CIRCUIT_BREAKER_RECOVERY_TIMEOUT=15m\nexport CIRCUIT_BREAKER_HALF_OPEN_MAX_REQUESTS=2\nexport CIRCUIT_BREAKER_RATE_LIMIT_PER_MINUTE=10\nexport CIRCUIT_BREAKER_MAX_CONCURRENT_INSTANCES=5\n</code></pre>"},{"location":"circuit-breaker/#available-presets","title":"Available Presets","text":""},{"location":"circuit-breaker/#conservative-production","title":"Conservative (Production)","text":"<p><pre><code>circuitBreaker:\n  preset: \"conservative\"\n</code></pre> - Failure Threshold: 2 - Rate Limit: 2/minute - Recovery: 20 minutes - Best for production with strict SLAs</p>"},{"location":"circuit-breaker/#balanced-default","title":"Balanced (Default)","text":"<p><pre><code>circuitBreaker:\n  preset: \"balanced\"\n</code></pre> - Failure Threshold: 3 - Rate Limit: 5/minute - Recovery: 15 minutes - Good balance for most workloads</p>"},{"location":"circuit-breaker/#aggressive","title":"Aggressive","text":"<p><pre><code>circuitBreaker:\n  preset: \"aggressive\"\n</code></pre> - Failure Threshold: 5 - Rate Limit: 10/minute - Recovery: 5 minutes - Higher throughput for development</p>"},{"location":"circuit-breaker/#demo","title":"Demo","text":"<p><pre><code>circuitBreaker:\n  preset: \"demo\"\n</code></pre> - Failure Threshold: 10 - Rate Limit: 20/minute - Recovery: 1 minute - Optimized for demonstrations</p>"},{"location":"circuit-breaker/#circuit-breaker-states","title":"Circuit Breaker States","text":""},{"location":"circuit-breaker/#closed-normal-operation","title":"CLOSED (Normal Operation)","text":"<ul> <li>All provisioning requests allowed</li> <li>Failures are tracked but don't block requests</li> <li>Rate limiting and concurrency limits still apply</li> </ul>"},{"location":"circuit-breaker/#open-failing-fast","title":"OPEN (Failing Fast)","text":"<ul> <li>All provisioning requests blocked</li> <li>Returns <code>CircuitBreakerError</code> immediately</li> <li>Automatically transitions to HALF_OPEN after <code>RecoveryTimeout</code></li> </ul>"},{"location":"circuit-breaker/#half_open-testing-recovery","title":"HALF_OPEN (Testing Recovery)","text":"<ul> <li>Limited number of test requests allowed (<code>HalfOpenMaxRequests</code>)</li> <li>Success \u2192 transitions to CLOSED</li> <li>Failure \u2192 transitions back to OPEN</li> </ul>"},{"location":"circuit-breaker/#protection-mechanisms","title":"Protection Mechanisms","text":""},{"location":"circuit-breaker/#1-rate-limiting","title":"1. Rate Limiting","text":"<pre><code>// Prevents more than N instances per minute\nif cb.instancesThisMinute &gt;= cb.config.RateLimitPerMinute {\n    return &amp;RateLimitError{\n        Limit:       cb.config.RateLimitPerMinute,\n        Current:     cb.instancesThisMinute,\n        TimeToReset: time.Minute - time.Since(cb.lastMinuteReset),\n    }\n}\n</code></pre>"},{"location":"circuit-breaker/#2-concurrency-control","title":"2. Concurrency Control","text":"<pre><code>// Prevents too many simultaneous provisioning operations\nif cb.concurrentInstances &gt;= cb.config.MaxConcurrentInstances {\n    return &amp;ConcurrencyLimitError{\n        Limit:   cb.config.MaxConcurrentInstances,\n        Current: cb.concurrentInstances,\n    }\n}\n</code></pre>"},{"location":"circuit-breaker/#3-failure-detection","title":"3. Failure Detection","text":"<pre><code>// Opens circuit after threshold failures within window\nif recentFailures &gt;= cb.config.FailureThreshold {\n    cb.transitionToOpen()\n}\n</code></pre>"},{"location":"circuit-breaker/#integration-with-cloudprovider","title":"Integration with CloudProvider","text":"<p>The circuit breaker is integrated into the main provisioning flow:</p> <pre><code>// Check circuit breaker before provisioning\nif err := c.circuitBreaker.CanProvision(ctx, nodeClass.Name, nodeClass.Spec.Region, 0); err != nil {\n    return nil, cloudprovider.NewInsufficientCapacityError(fmt.Errorf(\"circuit breaker blocked provisioning: %w\", err))\n}\n\n// Record success/failure after provisioning\nif err != nil {\n    c.circuitBreaker.RecordFailure(nodeClass.Name, nodeClass.Spec.Region, err)\n} else {\n    c.circuitBreaker.RecordSuccess(nodeClass.Name, nodeClass.Spec.Region)\n}\n</code></pre>"},{"location":"circuit-breaker/#error-types","title":"Error Types","text":""},{"location":"circuit-breaker/#circuitbreakererror","title":"CircuitBreakerError","text":"<pre><code>type CircuitBreakerError struct {\n    State      CircuitBreakerState\n    Message    string\n    TimeToWait time.Duration\n}\n</code></pre>"},{"location":"circuit-breaker/#ratelimiterror","title":"RateLimitError","text":"<pre><code>type RateLimitError struct {\n    Limit       int\n    Current     int\n    TimeToReset time.Duration\n}\n</code></pre>"},{"location":"circuit-breaker/#concurrencylimiterror","title":"ConcurrencyLimitError","text":"<pre><code>type ConcurrencyLimitError struct {\n    Limit   int\n    Current int\n}\n</code></pre>"},{"location":"circuit-breaker/#monitoring-and-observability","title":"Monitoring and Observability","text":""},{"location":"circuit-breaker/#circuit-breaker-status","title":"Circuit Breaker Status","text":"<pre><code>type CircuitBreakerStatus struct {\n    State               CircuitBreakerState\n    RecentFailures      int\n    FailureThreshold    int\n    InstancesThisMinute int\n    RateLimit           int\n    ConcurrentInstances int\n    MaxConcurrent       int\n    LastStateChange     time.Time\n    TimeToRecovery      time.Duration\n}\n</code></pre>"},{"location":"circuit-breaker/#logging","title":"Logging","text":"<p>The circuit breaker provides logging for: - State transitions - Provisioning attempts (allowed/blocked) - Success/failure recording - Rate limit and concurrency violations</p>"},{"location":"faq/","title":"Frequently Asked Questions","text":""},{"location":"faq/#how-does-karpenter-make-sizing-decisions-for-matching-pending-workloads-with-instance-types","title":"How does Karpenter make sizing decisions for matching pending workloads with instance types?","text":"<p>Karpenter uses amulti-step process to match pending pods with the most appropriate IBM Cloud instance types:</p>"},{"location":"faq/#1-instance-type-filtering","title":"1. Instance Type Filtering","text":"<p>Karpenter first filters available instance types based on pod requirements: - Architecture requirements (e.g., amd64, arm64) - Minimum CPU and memory requirements - Maximum hourly price constraints (if specified) - Zone availability in your region</p>"},{"location":"faq/#2-cost-efficiency-ranking","title":"2. Cost-Efficiency Ranking","text":"<p>Each viable instance type receives a cost-efficiency score calculated as: <pre><code>score = (price_per_CPU + price_per_GB_memory) / 2\n</code></pre> Lower scores indicate better cost efficiency. When pricing data is unavailable, Karpenter falls back to resource-based ranking, preferring smaller instances.</p>"},{"location":"faq/#3-bin-packing-optimization","title":"3. Bin Packing Optimization","text":"<p>The scheduler attempts to: - Pack multiple pending pods onto the smallest/cheapest instance that can accommodate them - Respect pod affinities, anti-affinities, and topology spread constraints - Honor taints/tolerations and node selectors - Account for system overhead and daemon resources</p>"},{"location":"faq/#how-does-karpenter-handle-multi-zone-deployments","title":"How does Karpenter handle multi-zone deployments?","text":"<p>Karpenter automatically discovers available zones in your IBM Cloud region and creates instance offerings across all zones.</p>"},{"location":"faq/#what-happens-if-pricing-information-is-unavailable","title":"What happens if pricing information is unavailable?","text":"<p>If the pricing provider cannot retrieve pricing data for an instance type, Karpenter falls back to resource-based ranking. It will prefer smaller instances (lower combined CPU + memory count) to ensure workloads can still be scheduled efficiently even without cost data.</p>"},{"location":"faq/#how-does-karpenter-decide-when-to-provision-a-new-node","title":"How does Karpenter decide when to provision a new node?","text":"<p>Karpenter watches for pods that cannot be scheduled due to insufficient resources. When it detects pending pods: 1. It evaluates if existing nodes can accommodate them 2. If not, it calculates the optimal new node configuration 3. It provisions the smallest/cheapest instance type that satisfies all requirements</p>"},{"location":"faq/#can-i-restrict-which-instance-types-karpenter-uses","title":"Can I restrict which instance types Karpenter uses?","text":"<p>Yes, you can control instance type selection through your NodePool configuration: - Use <code>instanceTypes</code> to explicitly list allowed instance types - Set <code>minCPU</code>, <code>minMemory</code>, and <code>maxPrice</code> requirements - Use taints and node selectors to influence placement</p>"},{"location":"faq/#how-does-karpenter-handle-daemonsets","title":"How does Karpenter handle DaemonSets?","text":"<p>Karpenter automatically accounts for DaemonSet resource requirements when calculating available capacity on nodes. It ensures that nodes have sufficient resources for both DaemonSets and regular pods.</p>"},{"location":"faq/#whats-the-difference-between-vpc-and-iks-modes","title":"What's the difference between VPC and IKS modes?","text":"<ul> <li>VPC Mode: Used for self-managed Kubernetes clusters on IBM Cloud VPC. Karpenter directly provisions Virtual Server Instances and manages the complete node lifecycle.</li> <li>IKS Mode: Used with IBM Kubernetes Service. Karpenter scales existing worker pools rather than creating individual instances.</li> </ul>"},{"location":"faq/#does-karpenter-support-gpu-instances","title":"Does Karpenter support GPU instances?","text":"<p>Yes, Karpenter recognizes and properly accounts for GPU resources in IBM Cloud instance types. It will match pods requesting GPU resources with appropriate GPU-enabled instances.</p>"},{"location":"faq/#how-does-node-consolidation-work","title":"How does node consolidation work?","text":"<p>Karpenter continuously evaluates node utilization and automatically: - Removes nodes that are empty or underutilized - Consolidates workloads onto fewer nodes when possible - Respects PodDisruptionBudgets during consolidation</p>"},{"location":"getting-started/","title":"Getting Started with Karpenter IBM Cloud Provider","text":"<p>This guide walks you through setting up Karpenter IBM Cloud Provider from installation to your first auto-scaled workload.</p>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<p>Before starting, ensure you have:</p>"},{"location":"getting-started/#required-access","title":"Required Access","text":"<ul> <li>IBM Cloud Account with VPC Infrastructure Services access</li> <li>Kubernetes Cluster (IKS or self-managed on IBM Cloud VPC)</li> <li>kubectl configured for your cluster</li> <li>Helm 3 for installation</li> </ul>"},{"location":"getting-started/#required-tools","title":"Required Tools","text":"<pre><code># Verify tools are available\nkubectl version --client\nhelm version\nibmcloud version\n\n# Install IBM Cloud CLI if needed\ncurl -fsSL https://clis.cloud.ibm.com/install/linux | sh\nibmcloud plugin install vpc-infrastructure\n</code></pre>"},{"location":"getting-started/#ibm-cloud-setup","title":"IBM Cloud Setup","text":""},{"location":"getting-started/#step-1-create-service-id-and-api-keys","title":"Step 1: Create Service ID and API Keys","text":"<p>For production environments, use Service IDs for better security:</p> <pre><code># Login to IBM Cloud\nibmcloud login\n\n# Create Service ID for Karpenter\nibmcloud iam service-id-create karpenter-provider \\\n  --description \"Service ID for Karpenter IBM Cloud Provider\"\n\n# Get Service ID\nSERVICE_ID=$(ibmcloud iam service-ids --output json | jq -r '.[] | select(.name==\"karpenter-provider\") | .id')\n\n# Assign VPC Infrastructure Services role\nibmcloud iam service-policy-create $SERVICE_ID \\\n  --roles \"VPC Infrastructure Services\" \\\n  --service-name is\n\n# Create API keys\nibmcloud iam service-api-key-create karpenter-general $SERVICE_ID \\\n  --description \"General IBM Cloud API access for Karpenter\"\n\nibmcloud iam service-api-key-create karpenter-vpc $SERVICE_ID \\\n  --description \"VPC-specific API access for Karpenter\"\n</code></pre> <p>Save the API keys securely - they won't be shown again!</p>"},{"location":"getting-started/#step-2-gather-required-resource-information","title":"Step 2: Gather Required Resource Information","text":"<pre><code># Set your target region\nexport REGION=us-south\nibmcloud target -r $REGION\n\n# List available VPCs\nibmcloud is vpcs --output json\n\n# Choose your VPC and list subnets\nexport VPC_ID=\"your-vpc-id\"\nibmcloud is subnets --vpc $VPC_ID --output json\n\n# List security groups in your VPC\nibmcloud is security-groups --vpc $VPC_ID --output json\n\n# List available images\nibmcloud is images --visibility public --status available | grep ubuntu\n</code></pre> <p>Collect the following information:</p> <ul> <li>VPC ID: Must be an ID like <code>r006-4225852b-4846-4a4a-88c4-9966471337c6</code></li> <li>Subnet ID: Format <code>02c7-718345b5-2de1-4a9a-b1de-fa7e307ee8c5</code></li> <li>Security Group ID: Must be ID <code>r006-36f045e2-86a1-4af8-917e-b17a41f8abe3</code></li> <li>Image ID: Prefer ID <code>r006-dd3c20fa-71d3-4dc0-913f-2f097bf3e500</code> (names like \"ubuntu-22-04\" work as well)</li> <li>API Server Endpoint: Get your cluster's API endpoint (e.g., <code>https://10.240.0.1:6443</code>)</li> <li>Region: <code>us-south</code> (or your preferred region)</li> <li>Zone: <code>us-south-1</code> (subnet's availability zone)</li> <li>\ud83d\udca1 Multi-Zone Tip: For production deployments, consider using <code>placementStrategy</code> instead of explicit zones for automatic multi-zone distribution. See Multi-Zone VPC Setup with Placement Constraints</li> </ul>"},{"location":"getting-started/#installation","title":"Installation","text":""},{"location":"getting-started/#step-1-install-helm-chart","title":"Step 1: Install Helm Chart","text":"<p>Install directly with API keys as helm values:</p> <pre><code># Add Helm repository\nhelm repo add karpenter-ibm https://karpenter-ibm.sh\nhelm repo update\n\n# Install with your API keys\nhelm install karpenter karpenter-ibm/karpenter-ibm \\\n  --namespace karpenter \\\n  --create-namespace \\\n  --set credentials.region=\"us-south\" \\\n  --set credentials.ibmApiKey=\"your-general-api-key\" \\\n  --set credentials.vpcApiKey=\"your-vpc-api-key\"\n</code></pre>"},{"location":"getting-started/#step-2-verify-installation","title":"Step 2: Verify Installation","text":"<pre><code># Check if pods are running\nkubectl get pods -n karpenter\n\n# Check controller logs for startup\nkubectl logs -n karpenter deployment/karpenter -f\n\n# Look for successful controller startup messages\n# Expected: \"Starting Controller\" for nodepool, nodeclaim, nodeclass controllers\n</code></pre> <p>Expected Output: <pre><code>Starting Controller {\"controller\": \"nodepool\", \"controllerGroup\": \"karpenter.sh\"}\nStarting Controller {\"controller\": \"nodeclaim\", \"controllerGroup\": \"karpenter.sh\"}\nStarting Controller {\"controller\": \"nodeclass.hash\", \"controllerGroup\": \"karpenter-ibm.sh\"}\nStarting Controller {\"controller\": \"pricing\", \"controllerGroup\": \"karpenter-ibm.sh\"}\n</code></pre></p>"},{"location":"getting-started/#step-3-create-your-first-nodeclass","title":"Step 3: Create Your First NodeClass","text":"<p>Choose the configuration that matches your environment:</p> <ul> <li>IKS Integration - For IBM Kubernetes Service clusters</li> <li>VPC Integration - For self-managed clusters on IBM Cloud VPC</li> </ul>"},{"location":"iks-integration/","title":"IKS Integration Guide","text":"<p>This guide focuses specifically on using Karpenter IBM Cloud Provider with IBM Kubernetes Service (IKS) clusters.</p>"},{"location":"iks-integration/#overview","title":"Overview","text":"<p>The IKS integration provides experimental auto-scaling for IBM Kubernetes Service clusters by leveraging existing worker pools and IBM-managed infrastructure.</p>"},{"location":"iks-integration/#prerequisites","title":"Prerequisites","text":""},{"location":"iks-integration/#iks-cluster-requirements","title":"IKS Cluster Requirements","text":"<ul> <li>IKS Cluster: Running IBM Kubernetes Service cluster</li> <li>Worker Pools: Pre-configured worker pools for different instance types</li> <li>API Access: Service ID with IKS cluster access permissions</li> <li>Network Configuration: VPC with proper security groups</li> </ul>"},{"location":"iks-integration/#required-information","title":"Required Information","text":"<p>Gather the following before starting: <pre><code># Get your IKS cluster ID\nibmcloud ks clusters --provider vpc-gen2\n\n# List existing worker pools\nibmcloud ks worker-pools --cluster &lt;cluster-id&gt;\n\n# Get cluster details\nibmcloud ks cluster get --cluster &lt;cluster-id&gt;\n</code></pre></p>"},{"location":"iks-integration/#quick-setup","title":"Quick Setup","text":""},{"location":"iks-integration/#step-1-install-karpenter","title":"Step 1: Install Karpenter","text":"<pre><code># Create namespace and secrets\nkubectl create namespace karpenter\n\nkubectl create secret generic karpenter-ibm-credentials \\\n  --from-literal=ibmApiKey=\"your-general-api-key\" \\\n  --from-literal=vpcApiKey=\"your-vpc-api-key\" \\\n  --namespace karpenter\n\n# Install via Helm\nhelm repo add karpenter-ibm https://karpenter-ibm.sh\nhelm repo update\nhelm install karpenter karpenter-ibm/karpenter-ibm \\\n  --namespace karpenter \\\n  --create-namespace \\\n  --set controller.env.IBM_REGION=\"us-south\"\n</code></pre>"},{"location":"iks-integration/#step-2-create-iks-nodeclass","title":"Step 2: Create IKS NodeClass","text":"<pre><code>apiVersion: karpenter-ibm.sh/v1alpha1\nkind: IBMNodeClass\nmetadata:\n  name: iks-nodeclass\n  annotations:\n    karpenter-ibm.sh/description: \"IKS integration NodeClass\"\nspec:\n  # REQUIRED: Replace with your actual values\n  region: us-south                      # Your IBM Cloud region\n  vpc: vpc-iks-12345678                 # Your IKS cluster VPC\n  image: r006-12345678                  # Ubuntu 20.04 or cluster-compatible image\n\n  # IKS-SPECIFIC CONFIGURATION\n  bootstrapMode: iks-api                # Use IKS API for node bootstrapping\n  iksClusterID: \"cluster-12345678\"      # Your IKS cluster ID (required for iks-api mode)\n  iksWorkerPoolID: \"pool-default\"       # Optional: specific worker pool\n\n  # Security and networking\n  securityGroups:\n  - sg-iks-workers                      # IKS worker security group\n\n  # Optional: SSH access for troubleshooting\n  sshKeys:\n  - key-iks-access\n</code></pre>"},{"location":"iks-integration/#step-3-create-nodepool","title":"Step 3: Create NodePool","text":"<pre><code>apiVersion: karpenter.sh/v1\nkind: NodePool\nmetadata:\n  name: iks-nodepool\nspec:\n  template:\n    metadata:\n      labels:\n        provisioner: karpenter-iks\n        cluster-type: iks\n    spec:\n      nodeClassRef:\n        apiVersion: karpenter-ibm.sh/v1alpha1\n        kind: IBMNodeClass\n        name: iks-nodeclass\n\n      # Instance requirements (limited by worker pool configuration)\n      requirements:\n      - key: node.kubernetes.io/instance-type\n        operator: In\n        values: [\"bx2-4x16\"]  # Must match worker pool instance type\n      - key: kubernetes.io/arch\n        operator: In\n        values: [\"amd64\"]\n\n  limits:\n    cpu: 1000\n    memory: 1000Gi\n\n  disruption:\n    consolidationPolicy: WhenEmpty\n    consolidateAfter: 30s\n</code></pre>"},{"location":"iks-integration/#important-iks-constraints","title":"Important IKS Constraints","text":""},{"location":"iks-integration/#instance-type-limitations","title":"Instance Type Limitations","text":"<p>Critical: IKS mode cannot dynamically select instance types because worker pools have pre-configured instance types.</p> <p>See IKS Mode Instance Type Constraints for more details.</p>"},{"location":"iks-integration/#iks-specific-troubleshooting","title":"IKS-Specific Troubleshooting","text":""},{"location":"iks-integration/#common-iks-issues","title":"Common IKS Issues","text":""},{"location":"iks-integration/#worker-pool-not-found","title":"Worker Pool Not Found","text":"<pre><code># Verify worker pool exists\nibmcloud ks worker-pools --cluster &lt;cluster-id&gt;\n\n# Check worker pool details\nibmcloud ks worker-pool get --cluster &lt;cluster-id&gt; --worker-pool &lt;pool-id&gt;\n</code></pre>"},{"location":"iks-integration/#e3917-api-errors","title":"E3917 API Errors","text":"<pre><code># Check if CLI fallback is working\nkubectl logs -n karpenter deployment/karpenter | grep -i \"e3917\\|cli\"\n\n# Verify IBM Cloud CLI is available in container\nkubectl exec -n karpenter deployment/karpenter -- ibmcloud version\n</code></pre>"},{"location":"iks-integration/#instance-type-mismatches","title":"Instance Type Mismatches","text":"<pre><code># Check worker pool instance configuration\nibmcloud ks worker-pool get --cluster &lt;cluster-id&gt; --worker-pool &lt;pool-id&gt; --output json\n\n# Verify NodePool requirements match worker pool\nkubectl describe nodepool &lt;nodepool-name&gt;\n</code></pre>"},{"location":"iks-integration/#monitoring-iks-integration","title":"Monitoring IKS Integration","text":"<pre><code># Watch worker pool scaling\nibmcloud ks workers --cluster &lt;cluster-id&gt; --worker-pool &lt;pool-id&gt;\n\n# Monitor Karpenter events\nkubectl get events --field-selector reason=SuccessfulCreate\n\n# Check node registration\nkubectl get nodes -l karpenter.sh/provisioner-name=&lt;nodepool-name&gt;\n</code></pre>"},{"location":"image-selection/","title":"Image Selection in IBM Cloud Karpenter Provider","text":"<p>The IBM Cloud Karpenter Provider offers two methods for specifying node images: explicit image specification and semantic image selection. This document explains both approaches and when to use each.</p>"},{"location":"image-selection/#overview","title":"Overview","text":"<p>Traditionally, specifying node images required knowing exact image IDs or names, which frequently change as IBM Cloud releases new images. The ImageSelector feature allows you to specify image requirements semantically, and the system automatically selects the most recent image matching your criteria.</p>"},{"location":"image-selection/#image-specification-methods","title":"Image Specification Methods","text":""},{"location":"image-selection/#method-1-explicit-image-specification-traditional","title":"Method 1: Explicit Image Specification (Traditional)","text":"<p>Use this method when you need to use a specific image ID or name:</p> <pre><code>apiVersion: karpenter-ibm.sh/v1alpha1\nkind: IBMNodeClass\nmetadata:\n  name: explicit-image-example\nspec:\n  image: \"r006-72b27b5c-f4b0-48bb-b954-5becc7c1dcb8\"  # Specific image ID\n  # OR\n  # image: \"ibm-ubuntu-22-04-minimal-amd64-3\"          # Specific image name\n\n  region: \"us-south\"\n  vpc: \"r010-12345678-1234-5678-9abc-def012345678\"\n  securityGroups:\n    - \"r010-87654321-4321-4321-4321-210987654321\"\n</code></pre> <p>Pros: - Predictable and deterministic - Guaranteed image availability (if it exists) - Full control over exact image version</p> <p>Cons: - Requires manual updates when images are deprecated - Risk of provisioning failures if image becomes unavailable - Need to track IBM Cloud image lifecycle</p>"},{"location":"image-selection/#method-2-semantic-image-selection-new","title":"Method 2: Semantic Image Selection (New)","text":"<p>Use this method for automatic selection of the latest available image matching your requirements:</p> <pre><code>apiVersion: karpenter-ibm.sh/v1alpha1\nkind: IBMNodeClass\nmetadata:\n  name: semantic-image-example\nspec:\n  imageSelector:\n    os: \"ubuntu\"              # Required: Operating system\n    majorVersion: \"22\"        # Required: Major version\n    minorVersion: \"04\"        # Optional: Minor version (omit for latest)\n    architecture: \"amd64\"     # Optional: CPU architecture (default: amd64)\n    variant: \"minimal\"        # Optional: Image variant (minimal, server, etc.)\n\n  region: \"us-south\"\n  vpc: \"r010-12345678-1234-5678-9abc-def012345678\"\n  securityGroups:\n    - \"r010-87654321-4321-4321-4321-210987654321\"\n</code></pre> <p>Pros: - Automatically uses latest available images - Reduces maintenance overhead - Resilient to image deprecation - Future-proof configuration</p> <p>Cons: - Less predictable (image may change between deployments) - Requires understanding of IBM Cloud naming conventions</p>"},{"location":"image-selection/#imageselector-fields","title":"ImageSelector Fields","text":""},{"location":"image-selection/#required-fields","title":"Required Fields","text":"<ul> <li><code>os</code>: Operating system name</li> <li>Common values: <code>\"ubuntu\"</code>, <code>\"rhel\"</code>, <code>\"centos\"</code>, <code>\"rocky\"</code>, <code>\"fedora\"</code>, <code>\"debian\"</code>, <code>\"suse\"</code></li> <li> <p>Must be lowercase letters only</p> </li> <li> <p><code>majorVersion</code>: Major version number</p> </li> <li>Examples: <code>\"20\"</code> for Ubuntu 20.x, <code>\"8\"</code> for RHEL 8.x, <code>\"22\"</code> for Ubuntu 22.x</li> <li>Must be numeric only</li> </ul>"},{"location":"image-selection/#optional-fields","title":"Optional Fields","text":"<ul> <li><code>minorVersion</code>: Minor version number</li> <li>Examples: <code>\"04\"</code> for Ubuntu x.04, <code>\"10\"</code> for Ubuntu x.10</li> <li>If omitted, the latest available minor version is selected</li> <li> <p>Must be numeric only</p> </li> <li> <p><code>architecture</code>: CPU architecture</p> </li> <li>Valid values: <code>\"amd64\"</code>, <code>\"arm64\"</code>, <code>\"s390x\"</code></li> <li> <p>Default: <code>\"amd64\"</code></p> </li> <li> <p><code>variant</code>: Image variant or flavor</p> </li> <li>Common values: <code>\"minimal\"</code>, <code>\"server\"</code>, <code>\"cloud\"</code>, <code>\"base\"</code></li> <li>If omitted, any variant is considered with preference for <code>\"minimal\"</code></li> <li>Must be lowercase letters only</li> </ul>"},{"location":"image-selection/#image-selection-logic","title":"Image Selection Logic","text":"<p>When using <code>imageSelector</code>, the system:</p> <ol> <li>Filters all available images by the specified criteria</li> <li>Sorts matching images by:</li> <li>Minor version (highest first, if not specified in selector)</li> <li>Build number (highest first)</li> <li>Creation date (newest first)</li> <li>Selects the first (most recent) image from the sorted list</li> </ol>"},{"location":"limitations/","title":"Current Limitations and Constraints","text":"<p>This document outlines the current limitations, constraints, and known issues with the Karpenter IBM Cloud Provider.</p>"},{"location":"limitations/#ibm-cloud-platform-limitations","title":"IBM Cloud Platform Limitations","text":""},{"location":"limitations/#networking-constraints","title":"Networking Constraints","text":""},{"location":"limitations/#single-zone-per-nodeclass","title":"Single Zone per NodeClass","text":"<ul> <li>Limitation: Each IBMNodeClass can only specify one zone</li> <li>Impact: Cannot auto-balance across multiple zones in single NodeClass</li> <li>Workaround: Create multiple NodeClasses for different zones</li> </ul> <pre><code># Required: Separate NodeClass per zone\n---\napiVersion: karpenter-ibm.sh/v1alpha1\nkind: IBMNodeClass\nmetadata:\n  name: nodeclass-us-south-1\nspec:\n  zone: us-south-1\n---\napiVersion: karpenter-ibm.sh/v1alpha1\nkind: IBMNodeClass\nmetadata:\n  name: nodeclass-us-south-2\nspec:\n  zone: us-south-2\n</code></pre>"},{"location":"limitations/#no-multi-zone-auto-placement","title":"No Multi-Zone Auto-Placement","text":"<ul> <li>Status: Not implemented</li> <li>Impact: Manual zone specification required</li> </ul>"},{"location":"limitations/#storage-limitations","title":"Storage Limitations","text":""},{"location":"limitations/#limited-storage-integration","title":"Limited Storage Integration","text":"<ul> <li>Current: Basic boot volume support only</li> <li>Missing:</li> <li>Dynamic storage provisioning during node creation</li> <li>Multiple storage attachments</li> <li>Custom storage profiles</li> <li>Workaround: Configure storage post-provisioning via storage classes</li> </ul>"},{"location":"limitations/#no-instance-store-support","title":"No Instance Store Support","text":"<ul> <li>Status: Not implemented</li> <li>Impact: Cannot use local NVMe storage</li> <li>Alternative: Use IBM Cloud Block Storage</li> </ul>"},{"location":"limitations/#provider-specific-limitations","title":"Provider-Specific Limitations","text":""},{"location":"limitations/#bootstrap-mode-limitations","title":"Bootstrap Mode Limitations","text":""},{"location":"limitations/#iks-mode-instance-type-constraints","title":"IKS Mode Instance Type Constraints","text":"<ul> <li>Impact: When using IKS mode (when <code>iksClusterID</code> is specified or <code>bootstrapMode: \"iks-api\"</code>), the provisioner cannot dynamically select instance types based on pod requirements</li> <li>Root Cause: IKS Worker Pool Resize API (<code>PATCH /v1/clusters/{id}/workerpools/{poolId}</code>) adds nodes with instance types pre-configured in the worker pool</li> <li>Current Behavior:</li> <li><code>instanceProfile</code> and <code>instanceRequirements</code> fields in IBMNodeClass are ignored in IKS mode</li> <li>All new nodes use the instance type configured in the existing worker pool</li> <li>Workarounds:</li> <li>Pre-create separate worker pools for different instance types</li> <li>Use multiple NodeClasses targeting different worker pools</li> </ul> <pre><code># Example: Multiple NodeClasses for different instance types in IKS mode\n---\napiVersion: karpenter-ibm.sh/v1alpha1\nkind: IBMNodeClass\nmetadata:\n  name: small-instances\nspec:\n  iksClusterID: \"cluster-id\"\n  iksWorkerPoolID: \"worker-pool-small\"  # Pre-configured with small instances\n---\napiVersion: karpenter-ibm.sh/v1alpha1\nkind: IBMNodeClass\nmetadata:\n  name: large-instances\nspec:\n  iksClusterID: \"cluster-id\"\n  iksWorkerPoolID: \"worker-pool-large\"  # Pre-configured with large instances\n</code></pre>"},{"location":"limitations/#tagging-and-metadata","title":"Tagging and Metadata","text":""},{"location":"limitations/#basic-tagging-support","title":"Basic Tagging Support","text":"<ul> <li>Current: Limited tag management</li> <li>Missing:</li> <li>Tag propagation from NodePool to instances</li> <li>Dynamic tag updates</li> <li>Cost allocation tags</li> </ul>"},{"location":"limitations/#no-interruption-detection","title":"No Interruption Detection","text":"<ul> <li>Impact: Cannot preemptively handle instance interruptions</li> </ul>"},{"location":"limitations/#networking-features","title":"Networking Features","text":""},{"location":"limitations/#no-load-balancer-integration","title":"No Load Balancer Integration","text":"<ul> <li>Missing: Automatic load balancer target registration</li> <li>Impact: Manual load balancer configuration required</li> <li>Workaround: Use Kubernetes ingress controllers</li> </ul>"},{"location":"limitations/#limited-security-group-management","title":"Limited Security Group Management","text":"<ul> <li>Current: Uses default or specified security groups</li> <li>Missing: Dynamic security group creation and management</li> <li>Workaround: Pre-create security groups with required rules</li> </ul>"},{"location":"limitations/#integration-limitations","title":"Integration Limitations","text":""},{"location":"limitations/#getting-help-with-limitations","title":"Getting Help with Limitations","text":""},{"location":"limitations/#report-new-limitations","title":"Report New Limitations","text":"<p>If you encounter limitations not documented here:</p> <ol> <li>Check existing issues: GitHub Issues</li> <li>Create new issue</li> <li>Provide context</li> </ol>"},{"location":"limitations/#request-feature-priority","title":"Request Feature Priority","text":"<p>To prioritize development of specific features:</p> <ol> <li>Upvote existing issues: Show demand for features</li> <li>Comment with use case: Explain business impact</li> <li>Contribute: Submit PRs for high-priority features</li> </ol>"},{"location":"load-balancer-integration/","title":"Load Balancer Integration","text":"<p>The Karpenter IBM Cloud Provider supports automatic registration and deregistration of nodes with IBM Cloud Load Balancers. This feature enables seamless integration with existing load balancing infrastructure without requiring external controllers or manual configuration.</p>"},{"location":"load-balancer-integration/#overview","title":"Overview","text":"<p>When load balancer integration is enabled, the Karpenter provider will:</p> <ol> <li>Automatically register newly created nodes with specified load balancer pools</li> <li>Configure health checks according to your specifications</li> <li>Automatically deregister nodes when they are terminated or become unhealthy</li> <li>Validate configuration to ensure load balancers and pools exist before node creation</li> </ol>"},{"location":"load-balancer-integration/#configuration","title":"Configuration","text":"<p>Load balancer integration is configured in the <code>IBMNodeClass</code> specification under the <code>loadBalancerIntegration</code> field.</p>"},{"location":"load-balancer-integration/#basic-configuration","title":"Basic Configuration","text":"<pre><code>apiVersion: karpenter-ibm.sh/v1alpha1\nkind: IBMNodeClass\nmetadata:\n  name: lb-integrated-nodeclass\nspec:\n  # ... other nodeclass configuration ...\n\n  loadBalancerIntegration:\n    enabled: true\n    targetGroups:\n      - loadBalancerID: \"r010-12345678-1234-5678-9abc-def012345678\"\n        poolName: \"web-servers\"\n        port: 80\n</code></pre>"},{"location":"load-balancer-integration/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code>loadBalancerIntegration:\n  enabled: true\n  autoDeregister: true\n  registrationTimeout: 300\n\n  targetGroups:\n    # Multiple target groups supported\n    - loadBalancerID: \"r010-12345678-1234-5678-9abc-def012345678\"\n      poolName: \"web-servers\"\n      port: 80\n      weight: 50\n      healthCheck:\n        protocol: \"http\"\n        path: \"/health\"\n        interval: 30\n        timeout: 5\n        retryCount: 2\n\n    - loadBalancerID: \"r010-87654321-4321-8765-cba9-fedcba098765\"\n      poolName: \"api-servers\"\n      port: 8080\n      weight: 75\n      healthCheck:\n        protocol: \"https\"\n        path: \"/api/health\"\n        interval: 15\n        timeout: 3\n        retryCount: 3\n</code></pre>"},{"location":"load-balancer-integration/#configuration-reference","title":"Configuration Reference","text":""},{"location":"load-balancer-integration/#loadbalancerintegration","title":"LoadBalancerIntegration","text":"Field Type Required Default Description <code>enabled</code> boolean No <code>false</code> Enable/disable load balancer integration <code>autoDeregister</code> boolean No <code>true</code> Automatically remove nodes from load balancers when terminated <code>registrationTimeout</code> int32 No <code>300</code> Maximum time in seconds to wait for node registration <code>targetGroups</code> []LoadBalancerTarget No <code>[]</code> List of load balancer target groups to register with"},{"location":"load-balancer-integration/#loadbalancertarget","title":"LoadBalancerTarget","text":"Field Type Required Default Description <code>loadBalancerID</code> string Yes - IBM Cloud Load Balancer ID (format: <code>r010-xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx</code>) <code>poolName</code> string Yes - Name of the load balancer pool <code>port</code> int32 Yes - Port number on target instances (1-65535) <code>weight</code> int32 No <code>50</code> Load balancing weight (0-100) <code>healthCheck</code> LoadBalancerHealthCheck No - Health check configuration"},{"location":"load-balancer-integration/#loadbalancerhealthcheck","title":"LoadBalancerHealthCheck","text":"Field Type Required Default Description <code>protocol</code> string No <code>tcp</code> Health check protocol (<code>tcp</code>, <code>http</code>, <code>https</code>) <code>path</code> string No - URL path for HTTP/HTTPS health checks (required for HTTP/HTTPS) <code>interval</code> int32 No <code>30</code> Health check interval in seconds (5-300) <code>timeout</code> int32 No <code>5</code> Health check timeout in seconds (1-60) <code>retryCount</code> int32 No <code>2</code> Number of consecutive successful checks required (1-10)"},{"location":"load-balancer-integration/#examples","title":"Examples","text":""},{"location":"load-balancer-integration/#web-application-with-http-health-checks","title":"Web Application with HTTP Health Checks","text":"<pre><code>loadBalancerIntegration:\n  enabled: true\n  targetGroups:\n    - loadBalancerID: \"r010-12345678-1234-5678-9abc-def012345678\"\n      poolName: \"web-tier\"\n      port: 80\n      healthCheck:\n        protocol: \"http\"\n        path: \"/health\"\n        interval: 30\n        timeout: 5\n        retryCount: 2\n</code></pre>"},{"location":"load-balancer-integration/#api-service-with-https-health-checks","title":"API Service with HTTPS Health Checks","text":"<pre><code>loadBalancerIntegration:\n  enabled: true\n  targetGroups:\n    - loadBalancerID: \"r010-87654321-4321-8765-cba9-fedcba098765\"\n      poolName: \"api-tier\"\n      port: 8443\n      weight: 75\n      healthCheck:\n        protocol: \"https\"\n        path: \"/api/v1/health\"\n        interval: 15\n        timeout: 3\n        retryCount: 3\n</code></pre>"},{"location":"load-balancer-integration/#multiple-load-balancers","title":"Multiple Load Balancers","text":"<pre><code>loadBalancerIntegration:\n  enabled: true\n  autoDeregister: true\n  registrationTimeout: 300\n\n  targetGroups:\n    # Primary application load balancer\n    - loadBalancerID: \"r010-primary-lb-id\"\n      poolName: \"app-servers\"\n      port: 8080\n      weight: 50\n\n    # Metrics/monitoring load balancer\n    - loadBalancerID: \"r010-monitoring-lb-id\"\n      poolName: \"metrics-collection\"\n      port: 9090\n      weight: 100\n      healthCheck:\n        protocol: \"http\"\n        path: \"/metrics\"\n        interval: 60\n</code></pre>"},{"location":"load-balancer-integration/#prerequisites","title":"Prerequisites","text":"<p>Before enabling load balancer integration, ensure:</p> <ol> <li>Load balancers exist: The specified load balancer IDs must exist in your IBM Cloud account</li> <li>Pools are configured: The named pools must exist within the load balancers</li> <li>Proper permissions: The IBM Cloud API key used by Karpenter must have permissions to:</li> <li>Read load balancer details</li> <li>List and modify load balancer pool members</li> <li>Read and write load balancer pool configurations</li> </ol>"},{"location":"load-balancer-integration/#required-iam-permissions","title":"Required IAM Permissions","text":"<p>The service ID or user used by Karpenter needs the following IAM permissions:</p> <pre><code>{\n  \"roles\": [\n    {\n      \"role_id\": \"crn:v1:bluemix:public:iam::::role:Editor\",\n      \"type\": \"service\"\n    }\n  ],\n  \"resources\": [\n    {\n      \"attributes\": [\n        {\n          \"name\": \"serviceName\",\n          \"value\": \"is\"\n        },\n        {\n          \"name\": \"resourceType\",\n          \"value\": \"load-balancer\"\n        }\n      ]\n    }\n  ]\n}\n</code></pre>"},{"location":"nightly-builds/","title":"Nightly Builds","text":""},{"location":"nightly-builds/#overview","title":"Overview","text":"<p>Nightly builds are automatically created every day at 2 AM UTC from the latest commit on the main branch. These builds allow users to test the latest features and fixes before they are included in an official release.</p>"},{"location":"nightly-builds/#image-tags","title":"Image Tags","text":"<p>Nightly builds are published with two tags:</p> <ol> <li>Versioned nightly tag: <code>v&lt;latest-tag&gt;-&lt;date&gt;-&lt;commit-sha&gt;-nightly</code></li> <li> <p>Example: <code>v0.5.0-2024-01-15-a1b2c3d-nightly</code></p> </li> <li> <p>Latest nightly tag: <code>nightly</code></p> </li> <li>Always points to the most recent nightly build</li> </ol>"},{"location":"nightly-builds/#using-nightly-builds","title":"Using Nightly Builds","text":""},{"location":"nightly-builds/#pull-the-latest-nightly-build","title":"Pull the latest nightly build","text":"<pre><code>podman pull ghcr.io/kubernetes-sigs/karpenter-provider-ibm-cloud/controller:nightly\n</code></pre>"},{"location":"nightly-builds/#pull-a-specific-nightly-build","title":"Pull a specific nightly build","text":"<pre><code>podman pull ghcr.io/kubernetes-sigs/karpenter-provider-ibm-cloud/controller:v0.5.0-2024-01-15-a1b2c3d-nightly\n</code></pre>"},{"location":"nightly-builds/#deploy-with-helm-using-nightly-builds","title":"Deploy with Helm using nightly builds","text":"<pre><code>helm upgrade --install karpenter-ibm oci://ghcr.io/kubernetes-sigs/karpenter-provider-ibm-cloud/charts/karpenter-provider-ibm-cloud \\\n  --set controller.image.tag=nightly \\\n  --namespace karpenter \\\n  --create-namespace\n</code></pre>"},{"location":"nightly-builds/#build-schedule","title":"Build Schedule","text":"<ul> <li>Daily builds: Every day at 2 AM UTC</li> <li>Manual builds: Can be triggered manually through GitHub Actions</li> <li>Retention: Nightly builds older than 7 days are automatically cleaned up</li> </ul>"},{"location":"nightly-builds/#stability-notice","title":"Stability Notice","text":"<p>Important: Nightly builds are created from the latest code on the main branch and may contain:</p> <ul> <li>Unreleased features</li> <li>Breaking changes</li> <li>Bugs that haven't been discovered yet</li> </ul> <p>Nightly builds are intended for:</p> <ul> <li>Testing new features early</li> <li>Development and staging environments</li> <li>Contributing to the project</li> </ul> <p>Do not use nightly builds in production environments.</p>"},{"location":"presentations/","title":"Presentations and Talks","text":"<p>This page contains links to presentations, talks, and demos about the Karpenter IBM Cloud Provider.</p>"},{"location":"presentations/#conference-talks","title":"Conference Talks","text":""},{"location":"presentations/#devconfcz-2025","title":"DevConf.CZ 2025","text":""},{"location":"presentations/#beyond-cas-why-the-world-needs-another-kubernetes-cluster-autoscaler","title":"Beyond CAS: Why the world needs another Kubernetes Cluster Autoscaler","text":"<ul> <li>Speaker: Josephine Pfeffer</li> <li>Date: June 14, 2025</li> <li>Location: Brno, Czech Republic</li> <li>Resources: Recording</li> <li>Description: This talk introduces Karpenter as a modern alternative to Kubernetes' Cluster Autoscaler (CAS), designed for real-time, workload-specific node provisioning. Presents the development of a custom IBM Cloud Karpenter provider, highlighting architecture, implementation challenges, and experimental results. Uses scientific benchmarking to compare scaling efficiency, cost savings, and resource utilization between CAS and Karpenter across AWS and IBM Cloud in different scenarios.</li> </ul>"},{"location":"presentations/#demo-videos","title":"Demo Videos","text":""},{"location":"presentations/#interactive-demo-karpenter-ibm-cloud-provider-in-action","title":"Interactive Demo: Karpenter IBM Cloud Provider in Action","text":"<p>This interactive demo showcases the Karpenter IBM Cloud Provider performing automatic node provisioning and scaling based on pod demand.</p>"},{"location":"presentations/#community-talks","title":"Community Talks","text":""},{"location":"presentations/#sig-autoscaling-weekly-meeting","title":"SIG Autoscaling Weekly Meeting","text":""},{"location":"presentations/#provider-presentation-and-demo","title":"Provider Presentation and Demo","text":"<ul> <li>Date: September 11, 2025</li> <li>Meeting: Kubernetes SIG Autoscaling Weekly</li> <li>Resources: Recording</li> <li>Description: Presentation and demonstration of the Karpenter IBM Cloud Provider at the Kubernetes SIG Autoscaling weekly meeting.</li> </ul>"},{"location":"presentations/#contributing","title":"Contributing","text":"<p>If you've given a talk about the Karpenter IBM Cloud Provider or know of presentations that should be listed here, please:</p> <ol> <li>Open a pull request to add the presentation details</li> <li>Include the title, speaker, date, venue, and a brief description</li> <li>Add links to slides, videos, or recordings if available</li> </ol>"},{"location":"security-considerations/","title":"Security Considerations for Karpenter IBM Cloud Provider","text":""},{"location":"security-considerations/#overview","title":"Overview","text":"<p>This document outlines important security considerations when using the Karpenter IBM Cloud Provider. Users should be aware of these potential security risks and take appropriate measures to mitigate them.</p>"},{"location":"security-considerations/#attack-surfaces","title":"Attack Surfaces","text":""},{"location":"security-considerations/#1-user-data-and-bootstrap-scripts","title":"1. User Data and Bootstrap Scripts","text":"<p>The <code>IBMNodeClass</code> custom resource allows users to specify custom user data scripts that will be executed during node initialization. This is a powerful feature but also presents a significant security risk.</p> <p>Potential Risks: - Script Injection: Malicious scripts could be injected through user data - Privilege Escalation: Scripts run with root privileges during node bootstrap</p> <p>Recommendations: - Restrict Access: Use Kubernetes RBAC to strictly limit who can create/modify <code>IBMNodeClass</code> resources - Code Review: Always review user data scripts before deployment - Network Policies: Implement network policies to restrict outbound connections during bootstrap</p> <p>Example RBAC Configuration: <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: ibmnodeclass-admin\nrules:\n- apiGroups: [\"karpenter-ibm.sh\"]\n  resources: [\"ibmnodeclasses\"]\n  verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\", \"delete\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: ibmnodeclass-admin-binding\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: ibmnodeclass-admin\nsubjects:\n- kind: Group\n  name: system:masters  # Restrict to cluster admins only\n  apiGroup: rbac.authorization.k8s.io\n</code></pre></p>"},{"location":"security-considerations/#2-api-credentials","title":"2. API Credentials","text":"<p>The provider requires IBM Cloud API credentials to manage infrastructure resources.</p> <p>Potential Risks: - Credential Exposure: API keys in environment variables or configuration files - Insufficient Rotation: Long-lived credentials increase risk window - Broad Permissions: Over-privileged API keys</p> <p>Recommendations: - Implement Rotation: Regularly rotate API keys (recommended: every 90 days) - Principle of Least Privilege: Create API keys with minimal required permissions - Audit Access: Enable IBM Cloud audit logging for all API key usage</p> <p>Required IBM Cloud IAM Policies:</p> <p>The provider supports two deployment modes with different IAM requirements:</p> <p>For IKS (IBM Kubernetes Service) Deployments: <pre><code>{\n  \"roles\": [\n    {\n      \"role\": \"Operator\",\n      \"resources\": [\n        {\n          \"service\": \"containers-kubernetes\",\n          \"resourceType\": \"cluster\"\n        }\n      ]\n    },\n    {\n      \"role\": \"Viewer\",\n      \"resources\": [\n        {\n          \"service\": \"is\",\n          \"resourceType\": \"instance\"\n        }\n      ]\n    }\n  ]\n}\n</code></pre></p> <p>For VPC (Self-Managed Kubernetes) Deployments: <pre><code>{\n  \"roles\": [\n    {\n      \"role\": \"Editor\",\n      \"resources\": [\n        {\n          \"service\": \"is\",\n          \"resourceType\": \"instance\"\n        },\n        {\n          \"service\": \"is\",\n          \"resourceType\": \"subnet\"\n        },\n        {\n          \"service\": \"is\",\n          \"resourceType\": \"security-group\"\n        },\n        {\n          \"service\": \"is\",\n          \"resourceType\": \"image\"\n        },\n        {\n          \"service\": \"is\",\n          \"resourceType\": \"vpc\"\n        }\n      ]\n    },\n    {\n      \"role\": \"Viewer\",\n      \"resources\": [\n        {\n          \"service\": \"globalcatalog-collection\"\n        }\n      ]\n    }\n  ]\n}\n</code></pre></p> <p>Deployment Mode Differences: - IKS Mode: Used with managed IBM Kubernetes Service clusters. Requires <code>Operator</code> role for worker pool resizing and worker node management, plus <code>Viewer</code> access to VPC instances for worker-to-instance mapping. - VPC Mode: Used with self-managed Kubernetes clusters on IBM Cloud VPC. Requires full VPC resource management permissions for direct instance lifecycle management.</p>"},{"location":"supported-cni-cri/","title":"Supported CNI and CRI","text":"<p>This document outlines the supported Container Network Interface (CNI) plugins and Container Runtime Interface (CRI) implementations with the Karpenter IBM Cloud Provider.</p>"},{"location":"supported-cni-cri/#container-network-interface-cni-support","title":"Container Network Interface (CNI) Support","text":""},{"location":"supported-cni-cri/#tested-and-supported-cni-plugins","title":"Tested and Supported CNI Plugins","text":"CNI Plugin Status Bootstrap Mode Notes Calico \u2705 Fully Supported cloud-init, iks-api Default for most IBM Cloud deployments Cilium \u2705 Fully Supported cloud-init Advanced networking features supported Flannel \u2705 Basic Support cloud-init Simple overlay networking"},{"location":"supported-cni-cri/#cni-auto-detection","title":"CNI Auto-Detection","text":"<p>The Karpenter IBM Cloud Provider includes automatic CNI detection in cloud-init bootstrap mode:</p> <pre><code>apiVersion: karpenter-ibm.sh/v1alpha1\nkind: IBMNodeClass\nspec:\n  bootstrapMode: cloud-init  # Enables auto-detection\n  # CNI will be automatically detected and configured\n</code></pre> <p>Detection Process: 1. Scans cluster for CNI-specific DaemonSets and ConfigMaps 2. Identifies CNI plugin from <code>kube-system</code> namespace resources 3. Configures node networking accordingly 4. Falls back to generic CNI configuration if detection fails</p>"},{"location":"supported-cni-cri/#cni-specific-configuration","title":"CNI-Specific Configuration","text":""},{"location":"supported-cni-cri/#calico-configuration","title":"Calico Configuration","text":"<ul> <li>Detection: Looks for <code>calico-node</code> DaemonSet</li> <li>Configuration: Automatically configures IPPool and network policies</li> <li>Features: Full support for network policies and BGP routing</li> </ul>"},{"location":"supported-cni-cri/#cilium-configuration","title":"Cilium Configuration","text":"<ul> <li>Detection: Looks for <code>cilium</code> DaemonSet and ConfigMap</li> <li>Configuration: Configures eBPF datapath and cluster mesh</li> <li>Features: Advanced security policies and observability</li> </ul>"},{"location":"supported-cni-cri/#flannel-configuration","title":"Flannel Configuration","text":"<ul> <li>Detection: Looks for <code>kube-flannel</code> DaemonSet</li> <li>Configuration: Sets up VXLAN overlay networking</li> <li>Features: Basic pod-to-pod communication</li> </ul>"},{"location":"supported-cni-cri/#container-runtime-interface-cri-support","title":"Container Runtime Interface (CRI) Support","text":""},{"location":"supported-cni-cri/#supported-container-runtimes","title":"Supported Container Runtimes","text":"Runtime Status Detection Method Default Version containerd \u2705 Fully Supported Auto-detected 1.7.0+ CRI-O \ud83d\udd04 Basic Support Manual configuration 1.24+"},{"location":"supported-cni-cri/#runtime-auto-detection","title":"Runtime Auto-Detection","text":"<p>Automatic Detection Process: 1. Queries existing cluster nodes for runtime information 2. Reads <code>node.status.nodeInfo.containerRuntimeVersion</code> 3. Configures new nodes with the detected runtime 4. Falls back to containerd if detection fails</p> <p>Manual Runtime Configuration: <pre><code># Set via environment variable\nexport CONTAINER_RUNTIME=containerd\n\n# Or configure in bootstrap script\necho \"CONTAINER_RUNTIME=containerd\" &gt;&gt; /etc/environment\n</code></pre></p>"},{"location":"supported-cni-cri/#custom-configuration","title":"Custom Configuration","text":""},{"location":"supported-cni-cri/#advanced-cni-configuration","title":"Advanced CNI Configuration","text":"<pre><code>apiVersion: karpenter-ibm.sh/v1alpha1\nkind: IBMNodeClass\nspec:\n  bootstrapMode: cloud-init\n  userData: |\n    #!/bin/bash\n    # Custom CNI configuration\n    curl -L -o /opt/cni/bin/my-cni https://releases.example.com/cni\n    chmod +x /opt/cni/bin/my-cni\n\n    # Custom CNI config\n    cat &gt; /etc/cni/net.d/10-mycni.conf &lt;&lt;EOF\n    {\n      \"cniVersion\": \"0.4.0\",\n      \"name\": \"mycni\",\n      \"type\": \"my-cni\"\n    }\n    EOF\n</code></pre>"},{"location":"supported-cni-cri/#runtime-configuration-override","title":"Runtime Configuration Override","text":"<pre><code>spec:\n  userData: |\n    #!/bin/bash\n    # Force specific container runtime\n    export CONTAINER_RUNTIME=cri-o\n\n    # Install CRI-O\n    curl -L -o /tmp/crio.tar.gz https://releases.cri-o.io/...\n    tar -xzf /tmp/crio.tar.gz -C /\n    systemctl enable --now crio\n</code></pre>"},{"location":"troubleshooting/","title":"Troubleshooting Guide","text":"<p>This guide helps diagnose and resolve common issues with the Karpenter IBM Cloud Provider.</p>"},{"location":"troubleshooting/#quick-diagnostics","title":"Quick Diagnostics","text":""},{"location":"troubleshooting/#check-controller-status","title":"Check Controller Status","text":"<pre><code># Check if Karpenter controller is running\nkubectl get pods -n karpenter\n\n# Check controller logs\nkubectl logs -n karpenter deployment/karpenter -f\n\n# Check controller startup messages\nkubectl logs -n karpenter deployment/karpenter | grep \"Starting Controller\"\n</code></pre>"},{"location":"troubleshooting/#common-issues","title":"Common Issues","text":""},{"location":"troubleshooting/#authentication-issues","title":"Authentication Issues","text":"<p>Failed to authenticate with IBM Cloud API</p> <pre><code>Failed to create VPC client: authentication failed\nError: {\"errorMessage\":\"Unauthorized\",\"errorCode\":\"401\"}\n</code></pre> <p>Solution:</p> <ol> <li>Verify API keys are correctly set</li> <li>Check Service ID permissions</li> <li>Update Kubernetes secret: <pre><code>kubectl create secret generic karpenter-ibm-credentials \\\n  --from-literal=api-key=\"$IBM_API_KEY\" \\\n  --from-literal=vpc-api-key=\"$VPC_API_KEY\" \\\n  --namespace karpenter --dry-run=client -o yaml | kubectl apply -f -\n</code></pre></li> </ol>"},{"location":"troubleshooting/#instance-provisioning-issues","title":"Instance Provisioning Issues","text":"<p>No suitable subnets found</p> <p>Diagnosis: <pre><code># Check available subnets\nibmcloud is subnets --output json\n\n# Check subnet capacity\nibmcloud is subnet SUBNET_ID --output json\n</code></pre></p> <p>Solutions:</p> <ul> <li>Verify subnet exists in specified zone</li> <li>Ensure subnet has available IP addresses</li> <li>Consider using auto-subnet selection</li> </ul>"},{"location":"troubleshooting/#node-registration-issues","title":"Node Registration Issues","text":"<p>Nodes not joining cluster after provisioning</p> <p>This is often caused by a chain of issues. Work through this systematic checklist:</p>"},{"location":"troubleshooting/#1-verify-instance-creation","title":"1. Verify Instance Creation","text":"<pre><code># Check if instances are being created\nibmcloud is instances --output json | jq '.[] | select(.name | contains(\"nodepool\"))'\n\n# Check NodeClaim status\nkubectl get nodeclaims -o wide\nkubectl describe nodeclaim NODECLAIM_NAME\n</code></pre> <p>Expected: Instance status <code>running</code>, NodeClaim shows <code>Launched: True</code></p>"},{"location":"troubleshooting/#2-check-network-connectivity-most-common-issue","title":"2. Check Network Connectivity (Most Common Issue)","text":"<p>Step 2a: Verify Subnet Placement <pre><code># Find which subnet your cluster nodes are in\nkubectl get nodes -o wide  # Note the INTERNAL-IP range\n\n# Check if Karpenter nodes are in the same subnet\nibmcloud is instance INSTANCE_ID --output json | jq '.primary_network_interface.subnet'\n\n# If different subnets, nodes may be network-isolated!\n</code></pre></p> <p>Step 2b: Verify API Server Endpoint Configuration <pre><code># Find the INTERNAL API endpoint (not external!)\nkubectl get endpoints kubernetes -o yaml\n# OR\nkubectl get endpointslice -n default -l kubernetes.io/service-name=kubernetes\n\n# Check what's configured in IBMNodeClass\nkubectl get ibmnodeclass YOUR-NODECLASS -o yaml | grep apiServerEndpoint\n\n# Update if using external IP instead of internal\nkubectl patch ibmnodeclass YOUR-NODECLASS --type='merge' \\\n  -p='{\"spec\":{\"apiServerEndpoint\":\"https://INTERNAL-IP:6443\"}}'\n</code></pre></p> <p>Step 2c: Test Connectivity from Node <pre><code># Attach floating IP for debugging\n\n# Then SSH and test\nssh -i ~/.ssh/eb root@FLOATING_IP\n\n# Test network layers\nping INTERNAL_API_IP                          # Test ICMP\ntelnet INTERNAL_API_IP 6443                   # Test TCP\ncurl -k https://INTERNAL_API_IP:6443/healthz  # Test HTTPS\n</code></pre></p>"},{"location":"troubleshooting/#3-verify-security-groups","title":"3. Verify Security Groups","text":"<p>Security Group Requirements</p> <p>Both worker and control plane security groups need proper rules for bidirectional communication.</p> <p>Required Security Group Rules:</p> <pre><code># Check current security groups on instance\nibmcloud is instance INSTANCE_ID --output json | \\\n  jq '.network_interfaces[0].security_groups'\n\n# Worker Node Security Group needs:\n# Outbound rules\n- TCP 6443 to control plane subnet (Kubernetes API)\n- TCP 10250 to all nodes (Kubelet)\n- TCP/UDP 53 to 0.0.0.0/0 (DNS)\n- TCP 80,443 to 0.0.0.0/0 (Package downloads)\n\n# Inbound rules\n- TCP 6443 from control plane (API server callbacks)\n- TCP 10250 from all nodes (Kubelet peer communication)\n\n# Add missing rules example:\nibmcloud is security-group-rule-add WORKER_SG_ID \\\n  outbound tcp --port-min 6443 --port-max 6443 \\\n  --remote CONTROL_PLANE_SUBNET_CIDR\n\nibmcloud is security-group-rule-add WORKER_SG_ID \\\n  inbound tcp --port-min 6443 --port-max 6443 \\\n  --remote CONTROL_PLANE_SUBNET_CIDR\n</code></pre>"},{"location":"troubleshooting/#4-debug-bootstrap-process","title":"4. Debug Bootstrap Process","text":"<p>Check Cloud-Init Status: <pre><code># SSH to node (after attaching floating IP)\nssh -i ~/.ssh/eb root@FLOATING_IP\n\n# Check cloud-init progress\nsudo cloud-init status --long\n\n# View bootstrap logs\nsudo tail -100 /var/log/cloud-init.log\nsudo tail -100 /var/log/cloud-init-output.log\nsudo cat /var/log/karpenter-bootstrap.log\n\n# Check if kubelet was installed\nsudo systemctl status kubelet\nsudo journalctl -u kubelet --no-pager -n 50\n</code></pre></p> <p>Common Bootstrap Issues: - Package repository access blocked (check security groups for HTTP/HTTPS) - CNI conflicts (check for pre-existing CNI configurations)</p>"},{"location":"troubleshooting/#5-verify-ibmnodeclass-configuration","title":"5. Verify IBMNodeClass Configuration","text":"<pre><code># Check for common configuration issues\nkubectl get ibmnodeclass YOUR-NODECLASS -o yaml\n\n# Key fields to verify:\n# - apiServerEndpoint: Must be INTERNAL cluster endpoint\n# - bootstrapMode: Should be \"cloud-init\" for VPC\n# - securityGroups: Must include proper security group IDs\n# - sshKeys: Must use SSH key IDs (r010-xxx format), not names\n</code></pre>"},{"location":"troubleshooting/#6-check-resource-group-configuration","title":"6. Check Resource Group Configuration","text":"<pre><code># Verify instances are created in correct resource group\nibmcloud is instances --output json | \\\n  jq '.[] | select(.name | contains(\"nodepool\")) |\n  {name: .name, resource_group: .resource_group.id}'\n\n# Should match the resource group in IBMNodeClass\nkubectl get ibmnodeclass YOUR-NODECLASS -o yaml | grep resourceGroupID\n</code></pre>"},{"location":"troubleshooting/#security-group-configuration","title":"Security Group Configuration","text":"<p>Kubernetes API Server Access</p> <p>Common Issue: Security groups blocking API server communication (TCP 6443)</p> <p>Symptoms: <pre><code># From worker node:\nping API_SERVER_IP              # \u2705 SUCCESS\ncurl https://API_SERVER_IP:6443 # \u274c TIMEOUT\n</code></pre></p> <p>Required Security Group Rules:</p> <p>Worker Node Security Group: <pre><code># Allow outbound to API server\nibmcloud is security-group-rule-add WORKER_SG_ID \\\n  outbound tcp --port-min 6443 --port-max 6443 \\\n  --remote CONTROL_PLANE_SUBNET_CIDR\n\n# Allow inbound for return traffic\nibmcloud is security-group-rule-add WORKER_SG_ID \\\n  inbound tcp --port-min 6443 --port-max 6443 \\\n  --remote CONTROL_PLANE_SUBNET_CIDR\n</code></pre></p> <p>Control Plane Security Group: <pre><code># Allow inbound from workers\nibmcloud is security-group-rule-add CONTROL_PLANE_SG_ID \\\n  inbound tcp --port-min 6443 --port-max 6443 \\\n  --remote WORKER_SUBNET_CIDR\n</code></pre></p> <p>Debug connectivity: <pre><code># Test layer by layer\nping API_SERVER_IP                    # ICMP connectivity\ntelnet API_SERVER_IP 6443            # TCP connectivity\ncurl -k https://API_SERVER_IP:6443/healthz  # Application layer\n</code></pre></p>"},{"location":"troubleshooting/#debug-mode","title":"Debug Mode","text":"<p>Enable debug logging for detailed information:</p> <pre><code>apiVersion: v1\nkind: Deployment\nmetadata:\n  name: karpenter\n  namespace: karpenter\nspec:\n  template:\n    spec:\n      containers:\n      - name: controller\n        env:\n        - name: LOG_LEVEL\n          value: debug\n</code></pre>"},{"location":"troubleshooting/#getting-help","title":"Getting Help","text":"<ul> <li>GitHub Issues</li> </ul>"},{"location":"userdata-feature/","title":"UserData and UserDataAppend Feature Documentation","text":""},{"location":"userdata-feature/#overview","title":"Overview","text":"<p>The Karpenter IBM Cloud Provider supports multiple methods for customizing node initialization:</p> <ol> <li><code>userData</code> - Complete override of the bootstrap script</li> <li><code>userDataAppend</code> - Append custom commands to the standard bootstrap script</li> <li><code>BOOTSTRAP_*</code> Environment Variables - Inject credentials and configuration from Secrets</li> </ol>"},{"location":"userdata-feature/#usage-examples","title":"Usage Examples","text":""},{"location":"userdata-feature/#example-1-using-userdataappend","title":"Example 1: Using UserDataAppend","text":"<pre><code>apiVersion: karpenter-ibm.sh/v1alpha1\nkind: IBMNodeClass\nmetadata:\n  name: example-append\nspec:\n  region: us-south\n  vpc: r010-12345678-1234-5678-9abc-def012345678\n  image: ubuntu-24-04-amd64\n  securityGroups:\n    - r010-87654321-4321-8765-9abc-def098765432\n\n  # Append custom commands after bootstrap\n  userDataAppend: |\n    echo \"Running post-bootstrap configuration...\"\n    apt-get update &amp;&amp; apt-get install -y monitoring-tools\n    systemctl enable node-exporter\n    echo \"Configuration complete\"\n</code></pre>"},{"location":"userdata-feature/#example-2-using-userdata-complete-override","title":"Example 2: Using UserData (Complete Override)","text":"<pre><code>apiVersion: karpenter-ibm.sh/v1alpha1\nkind: IBMNodeClass\nmetadata:\n  name: example-override\nspec:\n  region: us-south\n  vpc: r010-12345678-1234-5678-9abc-def012345678\n  image: ubuntu-24-04-amd64\n  securityGroups:\n    - r010-87654321-4321-8765-9abc-def098765432\n\n  # Complete custom bootstrap script\n  userData: |\n    #!/bin/bash\n    set -euo pipefail\n\n    echo \"Custom bootstrap starting...\"\n    # You are responsible for:\n    # - Installing container runtime\n    # - Installing kubelet\n    # - Configuring CNI\n    # - Joining the cluster\n\n  # Note: userDataAppend is ignored when userData is set\n  userDataAppend: |\n    echo \"This will be ignored\"\n</code></pre>"},{"location":"userdata-feature/#bootstrap_-environment-variable-injection","title":"BOOTSTRAP_* Environment Variable Injection","text":""},{"location":"userdata-feature/#overview_1","title":"Overview","text":"<p>The Karpenter provider automatically injects environment variables prefixed with <code>BOOTSTRAP_*</code> from the operator deployment into userData scripts. This enables secure credential passing from Kubernetes Secrets without exposing sensitive data in IBMNodeClass CRDs.</p>"},{"location":"userdata-feature/#use-cases","title":"Use Cases","text":"<ul> <li>RKE2/K3s Bootstrapping: Pass server URLs and join tokens</li> <li>Custom Authentication: Inject API keys and certificates</li> <li>Configuration Management: Pass cluster-specific settings</li> <li>Secret Management: Reference values from Kubernetes Secrets</li> </ul>"},{"location":"userdata-feature/#how-it-works","title":"How It Works","text":"<pre><code>Karpenter Deployment \u2192 BOOTSTRAP_* env vars (with Secret refs)\n         \u2193\nCloud-init generation \u2192 Inject into userData\n         \u2193\nRendered script \u2192 IBM Cloud VPC API\n         \u2193\nInstance boots \u2192 Variables available in script\n</code></pre>"},{"location":"userdata-feature/#configuration-example","title":"Configuration Example","text":""},{"location":"userdata-feature/#step-1-create-a-secret","title":"Step 1: Create a Secret","text":"<pre><code>kubectl create secret generic rke2-join-config \\\n  --from-literal=server=https://10.20.5.71:9345 \\\n  --from-literal=token=K1060b31293... \\\n  -n karpenter\n</code></pre>"},{"location":"userdata-feature/#step-2-configure-karpenter-deployment","title":"Step 2: Configure Karpenter Deployment","text":"<pre><code># In Karpenter Helm values or deployment\nenv:\n  # Literal value\n  - name: BOOTSTRAP_RKE2_SERVER\n    value: \"https://10.20.5.71:9345\"\n\n  # From Secret\n  - name: BOOTSTRAP_RKE2_TOKEN\n    valueFrom:\n      secretKeyRef:\n        name: rke2-join-config\n        namespace: karpenter\n        key: token\n\n  # Optional version\n  - name: BOOTSTRAP_RKE2_VERSION\n    value: \"v1.30.2+rke2r1\"\n</code></pre>"},{"location":"userdata-feature/#step-3-use-in-userdata","title":"Step 3: Use in UserData","text":"<pre><code>apiVersion: karpenter-ibm.sh/v1alpha1\nkind: IBMNodeClass\nmetadata:\n  name: rke2-nodeclass\nspec:\n  region: us-south\n  vpc: r010-xxx\n  securityGroups: [r010-xxx]\n\n  userData: |\n    #!/bin/bash\n    set -euo pipefail\n\n    # BOOTSTRAP_* variables are automatically injected\n    echo \"Joining RKE2 cluster at: $BOOTSTRAP_RKE2_SERVER\"\n\n    # Install RKE2\n    curl -sfL https://get.rke2.io | \\\n        INSTALL_RKE2_VERSION=\"${BOOTSTRAP_RKE2_VERSION}\" \\\n        INSTALL_RKE2_TYPE=\"agent\" \\\n        sh -\n\n    # Configure with injected credentials\n    mkdir -p /etc/rancher/rke2\n    cat &gt; /etc/rancher/rke2/config.yaml &lt;&lt;EOF\n    server: ${BOOTSTRAP_RKE2_SERVER}\n    token: ${BOOTSTRAP_RKE2_TOKEN}\n    EOF\n\n    systemctl enable --now rke2-agent\n</code></pre>"},{"location":"userdata-feature/#complete-example-rke2-integration","title":"Complete Example: RKE2 Integration","text":"<p>See <code>examples/rke2-userdata.sh</code> for a complete working example.</p>"},{"location":"userdata-feature/#precedence-rules","title":"Precedence Rules","text":"<ol> <li>If <code>userData</code> is set:</li> <li>Use <code>userData</code> as the complete script</li> <li><code>BOOTSTRAP_*</code> variables are still injected</li> <li>Ignore <code>userDataAppend</code></li> <li> <p>Skip all Karpenter bootstrap logic</p> </li> <li> <p>If only <code>userDataAppend</code> is set:</p> </li> <li>Generate standard Karpenter bootstrap script</li> <li><code>BOOTSTRAP_*</code> variables are injected</li> <li> <p>Append custom commands at the end</p> </li> <li> <p>If neither is set:</p> </li> <li>Use standard Karpenter bootstrap script only</li> <li><code>BOOTSTRAP_*</code> variables are injected if defined</li> </ol>"},{"location":"vpc-integration/","title":"VPC Integration Guide","text":"<p>This guide focuses on using Karpenter IBM Cloud Provider with self-managed Kubernetes clusters running on IBM Cloud VPC infrastructure.</p>"},{"location":"vpc-integration/#overview","title":"Overview","text":"<p>VPC integration provides flexible node provisioning for self-managed Kubernetes clusters with full control over cluster configuration and automatic bootstrap capabilities.</p>"},{"location":"vpc-integration/#key-benefits","title":"Key Benefits","text":"<ul> <li>Automatic Bootstrap: Zero-configuration node joining with intelligent cluster discovery</li> <li>Dynamic Instance Selection: Full flexibility in instance type selection based on workload requirements</li> <li>Custom Configurations: Support for specialized setups (GPU, HPC, security hardening)</li> </ul>"},{"location":"vpc-integration/#prerequisites","title":"Prerequisites","text":""},{"location":"vpc-integration/#infrastructure-requirements","title":"Infrastructure Requirements","text":"<ul> <li>Self-Managed Kubernetes: Running on IBM Cloud VPC instances</li> <li>VPC Infrastructure: VPC with subnets, security groups, and network configuration</li> <li>API Access: Service ID with VPC Infrastructure Services permissions</li> <li>Network Connectivity: Proper security groups allowing cluster communication</li> </ul>"},{"location":"vpc-integration/#required-information","title":"Required Information","text":"<p>Gather the following before starting: <pre><code># List your VPCs\nibmcloud is vpcs --output json\n\n# List subnets in your VPC\nibmcloud is subnets --vpc &lt;vpc-id&gt; --output json\n\n# List security groups\nibmcloud is security-groups --vpc &lt;vpc-id&gt; --output json\n\n# List available images\nibmcloud is images --visibility public --status available | grep ubuntu\n</code></pre></p>"},{"location":"vpc-integration/#quick-setup","title":"Quick Setup","text":""},{"location":"vpc-integration/#step-1-install-karpenter","title":"Step 1: Install Karpenter","text":"<pre><code># Create namespace and secrets\nkubectl create namespace karpenter\n\nkubectl create secret generic karpenter-ibm-credentials \\\n  --from-literal=ibmApiKey=\"your-general-api-key\" \\\n  --from-literal=vpcApiKey=\"your-vpc-api-key\" \\\n  --namespace karpenter\n\n# Install via Helm\nhelm repo add karpenter-ibm https://karpenter-ibm.sh\nhelm repo update\nhelm install karpenter karpenter-ibm/karpenter-ibm \\\n  --namespace karpenter \\\n  --create-namespace \\\n  --set controller.env.IBM_REGION=\"us-south\"\n</code></pre>"},{"location":"vpc-integration/#step-2-create-vpc-nodeclass","title":"Step 2: Create VPC NodeClass","text":"<p>\u26a0\ufe0f CRITICAL CONFIGURATION REQUIREMENTS: - API Server Endpoint is REQUIRED - Without it, nodes cannot join the cluster - Use Resource IDs, NOT Names - Security groups, VPC, and subnet must be IDs - Bootstrap Mode - Explicitly set</p> <pre><code>apiVersion: karpenter-ibm.sh/v1alpha1\nkind: IBMNodeClass\nmetadata:\n  name: vpc-nodeclass\n  annotations:\n    karpenter-ibm.sh/description: \"VPC self-managed cluster NodeClass\"\nspec:\n  # REQUIRED: Replace with your actual resource IDs (NOT names!)\n  region: us-south                                        # Your IBM Cloud region\n  zone: us-south-1                                        # Target availability zone\n  vpc: \"r006-4225852b-4846-4a4a-88c4-9966471337c6\"       # VPC ID format: r###-########-####-####-####-############\n  image: \"r006-dd3c20fa-71d3-4dc0-913f-2f097bf3e500\"     # Image ID (recommended) or name\n\n  # CRITICAL: API Server Endpoint - nodes CANNOT join without this!\n  apiServerEndpoint: \"https://10.240.0.1:6443\"           # Your cluster's INTERNAL API endpoint\n\n  # REQUIRED: Bootstrap mode for VPC clusters\n  bootstrapMode: cloud-init                               # Valid: cloud-init, iks, user-data\n\n  # REQUIRED: Security groups (must be IDs, not names!)\n  securityGroups:\n    - \"r006-36f045e2-86a1-4af8-917e-b17a41f8abe3\"       # ID format: r###-########-####-####-####-############\n    # \u274c NOT \"sg-k8s-workers\" - names will cause validation errors!\n\n  # Optional: Specific subnet (must be ID if specified)\n  subnet: \"02c7-718345b5-2de1-4a9a-b1de-fa7e307ee8c5\"   # Format: ####-########-####-####-####-############\n\n  # Optional: Instance requirements for filtering available instance types\n  # If neither instanceRequirements nor instanceProfile are specified,\n  # instance types will be selected based on NodePool requirements\n  instanceRequirements:\n    architecture: amd64                 # CPU architecture: amd64, arm64, s390x\n    minimumCPU: 2                       # Minimum vCPUs required\n    minimumMemory: 4                    # Minimum memory in GiB\n    maximumHourlyPrice: \"1.00\"          # Maximum hourly price in USD\n\n  # Optional: Specific instance profile (mutually exclusive with instanceRequirements)\n  # instanceProfile: bx2-4x16           # Uncomment to use a specific instance type\n\n  # Optional: Placement strategy for multi-zone deployment (RECOMMENDED)\n  # Enables automatic subnet selection across zones with constraints\n  # See \"Multi-Zone VPC Setup with Placement Constraints\" section below for full examples\n  placementStrategy:\n    zoneBalance: Balanced               # Balanced, AvailabilityFirst, or CostOptimized\n    subnetSelection:                    # Optional: Subnet filtering constraints\n      minimumAvailableIPs: 10           # Minimum available IPs per subnet\n      requiredTags:                     # Required subnet tags\n        Environment: \"production\"       # Example: Only production subnets\n\n  # Optional: SSH access for troubleshooting\n  # To find SSH key IDs: ibmcloud is keys --output json | jq '.[] | {name, id}'\n  sshKeys:\n  - r010-12345678-1234-1234-1234-123456789012  # SSH key ID\n\n  # Resource group (supports both name and ID)\n  resourceGroup: my-resource-group       # Resource group name (automatically resolved to ID)\n  # OR:\n  # resourceGroup: rg-12345678          # Resource group ID (used directly)\n  # If omitted, instances are created in the account's default resource group\n\n  # Optional: Placement target (dedicated host or placement group)\n  placementTarget: ph-12345678\n\n  # Optional: Tags to apply to instances\n  tags:\n    environment: production\n    team: devops\n\n  # Optional: Bootstrap mode (cloud-init, iks-api, or auto)\n  bootstrapMode: cloud-init\n\n  # REQUIRED: Internal API server endpoint (find with: kubectl get endpointslice -n default -l kubernetes.io/service-name=kubernetes)\n  apiServerEndpoint: \"https://&lt;INTERNAL-API-SERVER-IP&gt;:6443\"\n\n  # Optional: IKS cluster ID (required when bootstrapMode is \"iks-api\")\n  iksClusterID: bng6n48d0t6vj7b33kag\n\n  # Optional: IKS worker pool ID (for IKS API bootstrapping)\n  iksWorkerPoolID: bng6n48d0t6vj7b33kag-pool1\n\n  # Optional: Load balancer integration\n  loadBalancerIntegration:\n    enabled: true\n    targetGroups:\n    - loadBalancerID: r010-12345678-1234-5678-9abc-def012345678\n      poolName: web-servers\n      port: 80\n      weight: 50\n    autoDeregister: true\n    registrationTimeout: 300\n\n  # VPC mode uses automatic bootstrap - no userData required!\n</code></pre>"},{"location":"vpc-integration/#step-3-create-nodepool","title":"Step 3: Create NodePool","text":"<pre><code>apiVersion: karpenter.sh/v1\nkind: NodePool\nmetadata:\n  name: vpc-nodepool\nspec:\n  template:\n    metadata:\n      labels:\n        provisioner: karpenter-vpc\n        cluster-type: self-managed\n    spec:\n      nodeClassRef:\n        apiVersion: karpenter-ibm.sh/v1alpha1\n        kind: IBMNodeClass\n        name: vpc-nodeclass\n\n      # Full flexibility in instance requirements\n      requirements:\n      - key: node.kubernetes.io/instance-type\n        operator: In\n        values: [\"bx2-2x8\", \"bx2-4x16\", \"cx2-2x4\", \"cx2-4x8\", \"mx2-2x16\"]\n      - key: kubernetes.io/arch\n        operator: In\n        values: [\"amd64\"]\n      - key: karpenter.sh/capacity-type\n        operator: In\n        values: [\"on-demand\"]\n\n  limits:\n    cpu: 1000\n    memory: 1000Gi\n\n  disruption:\n    consolidationPolicy: WhenEmpty\n    consolidateAfter: 30s\n</code></pre>"},{"location":"vpc-integration/#vpc-bootstrap-features","title":"VPC Bootstrap Features","text":""},{"location":"vpc-integration/#api-endpoint-discovery-critical-for-vpc-clusters","title":"API Endpoint Discovery (Critical for VPC Clusters)","text":"<p>Important: VPC clusters must use the internal API endpoint, not the external kubectl endpoint.</p>"},{"location":"vpc-integration/#finding-the-correct-internal-api-endpoint","title":"Finding the Correct Internal API Endpoint","text":"<pre><code># Method 1: Get actual API server endpoint (RECOMMENDED)\nkubectl get endpointslice -n default -l kubernetes.io/service-name=kubernetes\n\n# Example output:\n# NAME         ADDRESSTYPE   PORTS   ENDPOINTS       AGE\n# kubernetes   IPv4          6443    &lt;INTERNAL-IP&gt;   15d\n\n# Use: https://&lt;INTERNAL-IP&gt;:6443\n</code></pre> <pre><code># Method 2: Check kubernetes service (alternative)\nkubectl get svc kubernetes -o jsonpath='{.spec.clusterIP}'\n# Returns cluster IP (e.g., &lt;CLUSTER-IP&gt;) - use https://&lt;CLUSTER-IP&gt;:443\n</code></pre>"},{"location":"vpc-integration/#configuring-ibmnodeclass-with-correct-endpoint","title":"Configuring IBMNodeClass with Correct Endpoint","text":"<pre><code>apiVersion: karpenter-ibm.sh/v1alpha1\nkind: IBMNodeClass\nmetadata:\n  name: vpc-nodeclass\nspec:\n  # CRITICAL: Use INTERNAL endpoint from discovery above\n  apiServerEndpoint: \"https://&lt;INTERNAL-IP&gt;:6443\"\n\n  region: us-south\n  vpc: vpc-12345678\n  # ... rest of config\n</code></pre>"},{"location":"vpc-integration/#automatic-cluster-discovery","title":"Automatic Cluster Discovery","text":"<p>The VPC integration automatically discovers your cluster configuration:</p> <ul> <li>API Endpoint: Uses the internal cluster API server endpoint you configure</li> <li>CA Certificate: Extracts cluster CA certificate from existing nodes</li> <li>DNS Configuration: Discovers cluster DNS service IP and search domains</li> <li>Network Settings: Detects cluster pod and service CIDR ranges</li> <li>Runtime Detection: Matches container runtime used by existing nodes</li> </ul>"},{"location":"vpc-integration/#zero-configuration-bootstrap","title":"Zero Configuration Bootstrap","text":"<pre><code># Minimal configuration - everything else is automatic\napiVersion: karpenter-ibm.sh/v1alpha1\nkind: IBMNodeClass\nmetadata:\n  name: minimal-vpc\nspec:\n  region: us-south\n  zone: us-south-1\n  vpc: vpc-12345678\n  image: r006-ubuntu-20-04\n  securityGroups:\n  - sg-default\n  # No userData needed - bootstrap is fully automatic!\n</code></pre>"},{"location":"vpc-integration/#advanced-vpc-configurations","title":"Advanced VPC Configurations","text":""},{"location":"vpc-integration/#multi-zone-vpc-setup-with-placement-constraints","title":"Multi-Zone VPC Setup with Placement Constraints","text":"<p>Karpenter IBM Cloud Provider supports sophisticated multi-zone deployments with flexible constraints for zones and subnets. There are two approaches: PlacementStrategy (recommended) and explicit zone specifications.</p>"},{"location":"vpc-integration/#recommended-placementstrategy-for-automatic-multi-zone-distribution","title":"Recommended: PlacementStrategy for Automatic Multi-Zone Distribution","text":"<p>PlacementStrategy provides intelligent multi-zone distribution with automatic subnet selection based on constraints:</p> <pre><code>apiVersion: karpenter-ibm.sh/v1alpha1\nkind: IBMNodeClass\nmetadata:\n  name: production-multizone\n  annotations:\n    karpenter-ibm.sh/description: \"Production multi-zone with intelligent placement\"\nspec:\n  region: us-south\n  vpc: \"r006-4225852b-4846-4a4a-88c4-9966471337c6\"\n  image: \"r006-dd3c20fa-71d3-4dc0-913f-2f097bf3e500\"\n  securityGroups:\n    - \"r006-36f045e2-86a1-4af8-917e-b17a41f8abe3\"\n  apiServerEndpoint: \"https://10.240.0.1:6443\"\n  bootstrapMode: cloud-init\n\n  # \ud83c\udfaf PLACEMENT STRATEGY: Automatic multi-zone distribution\n  placementStrategy:\n    # Zone distribution strategy\n    zoneBalance: \"Balanced\"           # Options: Balanced, AvailabilityFirst, CostOptimized\n\n    # Subnet selection constraints\n    subnetSelection:\n      minimumAvailableIPs: 20         # Only subnets with \u226520 available IPs\n      requiredTags:                   # Tag-based subnet filtering\n        Environment: \"production\"     # Must have Environment=production\n        Tier: \"private\"              # Must have Tier=private\n        Team: \"platform\"             # Must have Team=platform\n---\napiVersion: karpenter.sh/v1\nkind: NodePool\nmetadata:\n  name: production-nodepool\nspec:\n  template:\n    spec:\n      nodeClassRef:\n        apiVersion: karpenter-ibm.sh/v1alpha1\n        kind: IBMNodeClass\n        name: production-multizone\n\n      # \ud83c\udfaf ZONE CONSTRAINTS: Limit which zones can be used\n      requirements:\n      - key: \"topology.kubernetes.io/zone\"\n        operator: In\n        values: [\"us-south-1\", \"us-south-2\"]    # Only allow these 2 zones\n      - key: node.kubernetes.io/instance-type\n        operator: In\n        values: [\"bx2-4x16\", \"bx2-8x32\", \"cx2-4x8\"]\n      - key: karpenter.sh/capacity-type\n        operator: In\n        values: [\"on-demand\"]\n\n  # Result: Nodes evenly distributed across us-south-1 and us-south-2,\n  # using only production/private/platform subnets with sufficient IPs\n</code></pre>"},{"location":"vpc-integration/#zone-balance-strategies","title":"Zone Balance Strategies","text":"<p>1. Balanced (Default) - Even distribution across all allowed zones: <pre><code>placementStrategy:\n  zoneBalance: \"Balanced\"\n# Result: 33% us-south-1, 33% us-south-2, 33% us-south-3\n</code></pre></p> <p>2. AvailabilityFirst - Prioritizes zones with highest availability: <pre><code>placementStrategy:\n  zoneBalance: \"AvailabilityFirst\"\n# Result: Prefers zones with most available capacity and best health metrics\n</code></pre></p> <p>3. CostOptimized - Balances cost efficiency with availability: <pre><code>placementStrategy:\n  zoneBalance: \"CostOptimized\"\n# Result: Considers instance pricing and availability zones for optimal cost-performance\n</code></pre></p>"},{"location":"vpc-integration/#how-multi-zone-constraints-work","title":"How Multi-Zone Constraints Work","text":"<ol> <li>Zone Filtering: NodePool requirements filter available zones (e.g., limit to us-south-1, us-south-2)</li> <li>Subnet Filtering: PlacementStrategy filters subnets by tags, available IPs, and state</li> <li>Zone Distribution: PlacementStrategy distributes filtered subnets across allowed zones</li> <li>Selection: Instance provider uses round-robin across distributed subnets</li> </ol>"},{"location":"vpc-integration/#explicit-zone-specifications","title":"Explicit Zone Specifications","text":"<p>For cases requiring explicit control, you can still specify zones and subnets directly:</p> <pre><code># Zone 1 NodeClass\n---\napiVersion: karpenter-ibm.sh/v1alpha1\nkind: IBMNodeClass\nmetadata:\n  name: vpc-us-south-1\nspec:\n  region: us-south\n  zone: us-south-1                    # Explicit zone\n  vpc: \"r006-vpc-id\"\n  subnet: \"02c7-subnet-zone1-id\"     # Explicit subnet\n  image: \"r006-image-id\"\n  securityGroups: [\"r006-sg-id\"]\n  apiServerEndpoint: \"https://10.240.0.1:6443\"\n  bootstrapMode: cloud-init\n---\n# Zone 2 NodeClass\napiVersion: karpenter-ibm.sh/v1alpha1\nkind: IBMNodeClass\nmetadata:\n  name: vpc-us-south-2\nspec:\n  region: us-south\n  zone: us-south-2                    # Explicit zone\n  vpc: \"r006-vpc-id\"\n  subnet: \"02c7-subnet-zone2-id\"     # Explicit subnet\n  image: \"r006-image-id\"\n  securityGroups: [\"r006-sg-id\"]\n  apiServerEndpoint: \"https://10.240.0.1:6443\"\n  bootstrapMode: cloud-init\n---\n# Separate NodePools for each zone\napiVersion: karpenter.sh/v1\nkind: NodePool\nmetadata:\n  name: zone1-nodepool\nspec:\n  template:\n    spec:\n      nodeClassRef:\n        name: vpc-us-south-1\n      requirements:\n      - key: \"topology.kubernetes.io/zone\"\n        operator: In\n        values: [\"us-south-1\"]\n</code></pre>"},{"location":"vpc-integration/#gpu-workloads","title":"GPU Workloads","text":"<pre><code>apiVersion: karpenter-ibm.sh/v1alpha1\nkind: IBMNodeClass\nmetadata:\n  name: vpc-gpu\nspec:\n  region: us-south\n  zone: us-south-1\n  vpc: vpc-gpu-12345\n  image: r006-ubuntu-20-04\n  instanceProfile: gx2-8x64x1v100      # GPU instance type\n  securityGroups:\n  - sg-gpu-workloads\n  userData: |\n    #!/bin/bash\n    # GPU drivers and configuration\n    apt-get update\n    apt-get install -y nvidia-driver-470 nvidia-container-toolkit\n\n    # Configure containerd for GPU support\n    mkdir -p /etc/containerd\n    cat &gt; /etc/containerd/config.toml &lt;&lt;EOF\n    [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.nvidia]\n      runtime_type = \"io.containerd.runc.v2\"\n      [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.nvidia.options]\n        BinaryName = \"/usr/bin/nvidia-container-runtime\"\n    EOF\n\n    # Bootstrap script automatically appended\n</code></pre>"},{"location":"vpc-integration/#high-performance-computing-hpc","title":"High-Performance Computing (HPC)","text":"<pre><code>apiVersion: karpenter-ibm.sh/v1alpha1\nkind: IBMNodeClass\nmetadata:\n  name: vpc-hpc\nspec:\n  region: us-south\n  zone: us-south-1\n  vpc: vpc-hpc-12345\n  image: r006-ubuntu-20-04\n  instanceProfile: cx2-32x64           # High-performance instance\n  securityGroups:\n  - sg-hpc-cluster\n  userData: |\n    #!/bin/bash\n    # HPC optimizations\n    echo performance &gt; /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor\n\n    # Memory optimizations\n    echo 'vm.swappiness = 1' &gt;&gt; /etc/sysctl.conf\n    echo 'vm.dirty_ratio = 15' &gt;&gt; /etc/sysctl.conf\n\n    # Install HPC libraries\n    apt-get update &amp;&amp; apt-get install -y \\\n      openmpi-bin openmpi-common libopenmpi-dev \\\n      libblas3 liblapack3\n\n    # Network optimizations for high-throughput\n    echo 'net.core.rmem_max = 134217728' &gt;&gt; /etc/sysctl.conf\n    echo 'net.core.wmem_max = 134217728' &gt;&gt; /etc/sysctl.conf\n</code></pre>"},{"location":"vpc-integration/#custom-cni-configuration","title":"Custom CNI Configuration","text":"<pre><code>apiVersion: karpenter-ibm.sh/v1alpha1\nkind: IBMNodeClass\nmetadata:\n  name: vpc-custom-cni\nspec:\n  region: us-south\n  zone: us-south-1\n  vpc: vpc-custom-12345\n  image: r006-ubuntu-20-04\n  userData: |\n    #!/bin/bash\n    # Custom CNI setup before cluster join\n\n    # Install Cilium CNI\n    curl -L -o /opt/cni/bin/cilium-cni \\\n      https://github.com/cilium/cilium/releases/download/v1.14.0/cilium-linux-amd64.tar.gz\n\n    # Custom CNI configuration\n    mkdir -p /etc/cni/net.d\n    cat &gt; /etc/cni/net.d/05-cilium.conf &lt;&lt;EOF\n    {\n      \"cniVersion\": \"0.4.0\",\n      \"name\": \"cilium\",\n      \"type\": \"cilium-cni\",\n      \"enable-debug\": false\n    }\n    EOF\n\n    # Bootstrap script handles the rest\n</code></pre>"},{"location":"vpc-integration/#dynamic-instance-selection","title":"Dynamic Instance Selection","text":"<p>Unlike IKS mode, VPC integration provides full flexibility in instance type selection. There are three ways to control instance selection:</p>"},{"location":"vpc-integration/#1-nodepool-requirements-recommended","title":"1. NodePool Requirements (Recommended)","text":"<p>When neither <code>instanceProfile</code> nor <code>instanceRequirements</code> are specified in the NodeClass, instance types are selected based on NodePool requirements. This provides maximum flexibility:</p>"},{"location":"vpc-integration/#2-nodeclass-instance-requirements","title":"2. NodeClass Instance Requirements","text":"<p>You can set filtering criteria in the NodeClass to limit available instance types globally for all NodePools using that NodeClass.</p>"},{"location":"vpc-integration/#3-nodeclass-specific-instance-profile","title":"3. NodeClass Specific Instance Profile","text":"<p>You can lock the NodeClass to a single instance type, though this limits flexibility.</p> <p>Example: NodePool-driven selection (most flexible approach):</p> <pre><code># NodeClass with NO instance specifications - lets NodePool control everything\napiVersion: karpenter-ibm.sh/v1alpha1\nkind: IBMNodeClass\nmetadata:\n  name: flexible-nodeclass\nspec:\n  region: us-south\n  vpc: \"r006-your-vpc-id\"\n  image: \"r006-your-image-id\"\n  securityGroups: [\"r006-your-sg-id\"]\n  apiServerEndpoint: \"https://10.240.0.1:6443\"\n  bootstrapMode: cloud-init\n  # Note: No instanceProfile or instanceRequirements specified\n---\napiVersion: karpenter.sh/v1\nkind: NodePool\nmetadata:\n  name: flexible-nodepool\nspec:\n  template:\n    spec:\n      nodeClassRef:\n        name: flexible-nodeclass\n      requirements:\n      # Karpenter will choose the best instance type based on pod requirements\n      - key: node.kubernetes.io/instance-type\n        operator: In\n        values: [\n          \"bx2-2x8\", \"bx2-4x16\", \"bx2-8x32\",    # Balanced instances\n          \"cx2-2x4\", \"cx2-4x8\", \"cx2-8x16\",     # Compute optimized\n          \"mx2-2x16\", \"mx2-4x32\", \"mx2-8x64\"    # Memory optimized\n        ]\n      - key: karpenter-ibm.sh/instance-family\n        operator: In\n        values: [\"bx2\", \"cx2\", \"mx2\"]\n      # Resource-based selection\n      - key: kubernetes.io/arch\n        operator: In\n        values: [\"amd64\"]\n      - key: karpenter.sh/capacity-type\n        operator: In\n        values: [\"on-demand\"]\n</code></pre>"},{"location":"vpc-integration/#vpc-specific-troubleshooting","title":"VPC-Specific Troubleshooting","text":""},{"location":"vpc-integration/#bootstrap-issues","title":"Bootstrap Issues","text":""},{"location":"vpc-integration/#wrong-api-endpoint-configuration","title":"Wrong API Endpoint Configuration","text":"<p>Symptoms: - NodeClaims created but nodes never register with cluster - Kubelet logs show: <code>\"Client.Timeout exceeded while awaiting headers\"</code> - Node status remains \"Unknown\" with \"Drifted\" = True</p> <p>Solution: <pre><code># 1. Find correct internal endpoint\nkubectl get endpointslice -n default -l kubernetes.io/service-name=kubernetes\n\n# 2. Update NodeClass with internal endpoint (NOT external)\nkubectl patch ibmnodeclass your-nodeclass --type='merge' \\\n  -p='{\"spec\":{\"apiServerEndpoint\":\"https://&lt;INTERNAL-IP&gt;:6443\"}}'\n\n# 3. Delete old NodeClaims to trigger new ones with correct config\nkubectl delete nodeclaims --all\n\n# 4. Monitor node registration\nkubectl get nodes -w\n</code></pre></p> <p>Verification: <pre><code># Test connectivity from worker instance\nssh ubuntu@&lt;node-ip&gt; \"telnet &lt;INTERNAL-IP&gt; 6443\"\n# Should connect successfully, not timeout\n\n# Check kubelet logs\nssh ubuntu@&lt;node-ip&gt; \"sudo journalctl -u kubelet -f\"\n# Should see successful API server communication\n</code></pre></p>"},{"location":"vpc-integration/#cluster-discovery-failures","title":"Cluster Discovery Failures","text":"<pre><code># Check if controller can reach cluster API\nkubectl logs -n karpenter deployment/karpenter | grep \"cluster discovery\"\n\n# Verify internal API endpoint is accessible\nssh ubuntu@&lt;node-ip&gt; \"curl -k https://INTERNAL_API_ENDPOINT/healthz\"\n\n# Check security group rules\nibmcloud is security-group &lt;sg-id&gt; --output json | jq '.rules'\n</code></pre>"},{"location":"vpc-integration/#bootstrap-script-problems","title":"Bootstrap Script Problems","text":"<pre><code># View generated bootstrap script\nssh ubuntu@&lt;node-ip&gt; \"sudo cat /var/lib/cloud/instance/scripts/*\"\n\n# Check cloud-init logs\nssh ubuntu@&lt;node-ip&gt; \"sudo journalctl -u cloud-final\"\n\n# Monitor bootstrap execution\nssh ubuntu@&lt;node-ip&gt; \"sudo tail -f /var/log/cloud-init-output.log\"\n</code></pre>"},{"location":"vpc-integration/#network-connectivity","title":"Network Connectivity","text":"<pre><code># Test cluster communication\nssh ubuntu@&lt;node-ip&gt; \"nc -zv CLUSTER_ENDPOINT 6443\"\n\n# Check DNS resolution\nssh ubuntu@&lt;node-ip&gt; \"nslookup kubernetes.default.svc.cluster.local\"\n\n# Verify route table\nibmcloud is vpc-routes &lt;vpc-id&gt;\n</code></pre>"},{"location":"vpc-integration/#instance-provisioning","title":"Instance Provisioning","text":"<pre><code># Check available instances in zone\nibmcloud is instance-profiles --output json | jq '.[] | select(.family==\"bx2\")'\n\n# Monitor quota usage\nibmcloud is instances --output json | jq 'length'\n\n# Check subnet capacity\nibmcloud is subnet &lt;subnet-id&gt; --output json | jq '.available_ipv4_address_count'\n</code></pre>"},{"location":"vpc-integration/#cni-initialization-timing-issues","title":"CNI Initialization Timing Issues","text":"<p>Problem: Newly provisioned nodes may be terminated prematurely before CNI (Calico) fully initializes.</p> <p>Symptoms: - Nodes created successfully but pods fail with CNI errors - <code>Failed to create pod sandbox: plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory</code> - Nodes get terminated and recreated in a loop</p> <p>Root Cause: Karpenter's <code>consolidateAfter</code> setting is too aggressive, terminating nodes before CNI initialization completes.</p> <p>Solution: <pre><code>apiVersion: karpenter.sh/v1\nkind: NodePool\nmetadata:\n  name: vpc-nodepool\nspec:\n  disruption:\n    consolidationPolicy: WhenEmpty\n    consolidateAfter: 300s  # Wait 5 minutes for CNI initialization\n</code></pre></p> <p>Verification: <pre><code># Check if Calico pods are running on new nodes\nkubectl get pods -n kube-system -o wide | grep calico-node\n\n# Monitor CNI status on a node\nssh ubuntu@&lt;node-ip&gt; \"sudo ls -la /var/lib/calico/\"\n\n# Check for CNI-related events\nkubectl get events --field-selector involvedObject.kind=Pod | grep -i sandbox\n</code></pre></p>"}]}